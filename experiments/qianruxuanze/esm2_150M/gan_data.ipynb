{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T15:40:45.446238Z",
     "start_time": "2025-07-14T15:40:45.437998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "# ========================\n",
    "# Config\n",
    "# ========================\n",
    "# 设置随机种子以确保可重复性\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seq_len = 300\n",
    "embed_dim = 640  # 修改为960维\n",
    "noise_dim = 128\n",
    "batch_size = 16\n",
    "n_critic = 5\n",
    "lambda_gp = 10\n",
    "epochs = 300\n",
    "checkpoint_dir = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 创建数据保存目录\n",
    "data_dir = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# ========== 判断是否已完成训练 ==========\n",
    "checkpoint_dir = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/\"\n",
    "epoch300_path = os.path.join(checkpoint_dir, \"generator_epoch300.pt\")\n",
    "best_path = os.path.join(checkpoint_dir, \"best_generator.pt\")\n",
    "\n",
    "if os.path.exists(epoch300_path) and os.path.exists(best_path):\n",
    "    print(\"[✓] 已检测到已完成训练的模型，将跳过训练阶段\")\n",
    "    skip_training = True\n",
    "else:\n",
    "    skip_training = False"
   ],
   "id": "bb6d554e4cb21a29",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T15:40:47.521337Z",
     "start_time": "2025-07-14T15:40:47.512789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================\n",
    "# Generator with Conv1d (保持原有架构完全不变)\n",
    "# ========================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(noise_dim, seq_len * 256)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(512, embed_dim, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z).view(-1, 256, seq_len)  # (B, 256, 300)\n",
    "        x = self.conv(x)  # (B, embed_dim, 300)\n",
    "        return x.transpose(1, 2)  # (B, 300, embed_dim)\n",
    "\n",
    "# ========================\n",
    "# Discriminator with Conv1d (保持原有架构完全不变)\n",
    "# ========================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 256, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 128, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * seq_len, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (B, embed_dim, 300)\n",
    "        x = self.conv(x)       # (B, 128, 300)\n",
    "        x = x.reshape(x.size(0), -1)  # (B, 128*300)\n",
    "        return self.fc(x)\n",
    "\n",
    "# =======================\n",
    "# Gradient Penalty (保持原有函数完全不变)\n",
    "# ========================\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, device=device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(d_interpolates.size(), device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
   ],
   "id": "59c6c33a3d3c6478",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T15:40:59.392496Z",
     "start_time": "2025-07-14T15:40:49.376764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# Load Real Data (修改为加载两个数据集并拼接)\n",
    "# ========================\n",
    "print(\"正在加载训练数据...\")\n",
    "train_data = np.load(\"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_train.npy\")\n",
    "print(\"训练数据 shape:\", train_data.shape)\n",
    "\n",
    "print(\"正在加载测试数据...\")\n",
    "test_data = np.load(\"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_test.npy\")\n",
    "print(\"测试数据 shape:\", test_data.shape)\n",
    "\n",
    "# 内存优化的数据拼接\n",
    "print(\"正在拼接数据...\")\n",
    "real_data = np.concatenate([train_data, test_data], axis=0)\n",
    "del train_data, test_data  # 释放内存\n",
    "\n",
    "print(\"拼接后数据 shape:\", real_data.shape)\n",
    "print(\"真实数据值域：\", real_data.min(), real_data.max())\n",
    "\n",
    "real_tensor = torch.tensor(real_data, dtype=torch.float32)\n",
    "dataloader = DataLoader(TensorDataset(real_tensor), batch_size=batch_size, shuffle=True)"
   ],
   "id": "853781e672982cc5",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:08:43.692719Z",
     "start_time": "2025-07-14T15:40:59.394307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================\n",
    "# Models (保持原有优化器参数完全不变)\n",
    "# ========================\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=1e-4, betas=(0.5, 0.9))  # 保持原有学习率和参数\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.9))  # 保持原有学习率和参数\n",
    "\n",
    "# =======================\n",
    "# Training (保持原有训练逻辑完全不变)\n",
    "# ========================\n",
    "best_g_loss = float(\"inf\")  # 初始化最小 G loss\n",
    "if not skip_training:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for i, (real,) in enumerate(pbar):\n",
    "            real = real.to(device)\n",
    "\n",
    "            # === Train D ===\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real.size(0), noise_dim).to(device)\n",
    "                fake = G(z).detach()\n",
    "                real_score = D(real)\n",
    "                fake_score = D(fake)\n",
    "                gp = compute_gradient_penalty(D, real, fake)\n",
    "                d_loss = -torch.mean(real_score) + torch.mean(fake_score) + lambda_gp * gp\n",
    "                optimizer_D.zero_grad()\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # === Train G ===\n",
    "            z = torch.randn(real.size(0), noise_dim).to(device)\n",
    "            fake = G(z)\n",
    "            g_loss = -torch.mean(D(fake))\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"D loss\": f\"{d_loss.item():.2f}\",\n",
    "                \"G loss\": f\"{g_loss.item():.2f}\"\n",
    "            })\n",
    "\n",
    "        # === 每 10 epoch 或最后一个 epoch保存模型 ===\n",
    "        if epoch % 100 == 0 or epoch == epochs:\n",
    "            save_path = os.path.join(checkpoint_dir, f\"generator_epoch{epoch}.pt\")\n",
    "            torch.save(G.state_dict(), save_path)\n",
    "            print(f\"[Checkpoint] Saved generator to {save_path}\")\n",
    "\n",
    "        # === 保存表现最好的 G ===\n",
    "        if g_loss.item() < best_g_loss:\n",
    "            best_g_loss = g_loss.item()\n",
    "            best_path = os.path.join(checkpoint_dir, \"best_generator.pt\")\n",
    "            torch.save(G.state_dict(), best_path)\n",
    "            print(f\"[BEST] Saved best generator with G loss = {best_g_loss:.4f}\")"
   ],
   "id": "ee9f4d9b11d5d6e",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:09:08.872594Z",
     "start_time": "2025-07-14T16:08:43.694816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# Save Model & Generate\n",
    "# ========================\n",
    "# ========================\n",
    "# ✅ 使用 best_generator 生成数据\n",
    "# ========================\n",
    "print(\"\\nLoading best generator for data generation...\")\n",
    "\n",
    "# 重新加载 best generator 权重\n",
    "G.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"generator_epoch300.pt\")))\n",
    "G.eval()\n",
    "\n",
    "# 生成 2435 条数据\n",
    "gen_total = 2435\n",
    "batch_size = 256\n",
    "generated = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    while total < gen_total:\n",
    "        current_batch = min(batch_size, gen_total - total)\n",
    "        z = torch.randn(current_batch, noise_dim).to(device)\n",
    "        fake = G(z).cpu().numpy()\n",
    "        generated.append(fake)\n",
    "        total += current_batch\n",
    "\n",
    "generated = np.concatenate(generated, axis=0)\n",
    "print(f\"生成了 {generated.shape[0]} 条数据，维度: {generated.shape[1:]}\")\n",
    "\n",
    "# 取前1948条数据\n",
    "generated_1948 = generated[:1948]\n",
    "print(f\"取前1948条数据，shape: {generated_1948.shape}\")\n",
    "\n",
    "# 加载原始训练数据用于拼接\n",
    "print(\"加载原始训练数据用于拼接...\")\n",
    "original_train_data = np.load(\"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_train.npy\")\n",
    "print(f\"原始训练数据 shape: {original_train_data.shape}\")\n",
    "\n",
    "# 拼接生成的数据和原始训练数据\n",
    "final_train_data = np.concatenate([original_train_data, generated_1948], axis=0)\n",
    "print(f\"最终训练数据 shape: {final_train_data.shape}\")\n",
    "\n",
    "# 保存最终训练集负样本\n",
    "save_data_path = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_train_embedding_enhanced.npy\"\n",
    "np.save(save_data_path, final_train_data)\n",
    "print(f\"已保存增强的训练集负样本到: {save_data_path}\")\n",
    "print(f\"数据维度: {final_train_data.shape}\")"
   ],
   "id": "d1560b986e352857",
   "execution_count": 9,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
