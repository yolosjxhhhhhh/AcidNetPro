{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T16:10:13.908452Z",
     "start_time": "2025-07-14T16:10:12.127833Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # 你可以换成其它数字\n",
    "# 数据集类\n",
    "class ProteinNPYDataset(Dataset):\n",
    "    def __init__(self, pos_path, neg_path):\n",
    "        self.pos = np.load(pos_path, mmap_mode='r')\n",
    "        self.neg = np.load(neg_path, mmap_mode='r')\n",
    "        self.lengths = [len(self.pos), len(self.neg)]\n",
    "        self.total_len = self.lengths[0] + self.lengths[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.lengths[0]:\n",
    "            x = self.pos[idx]\n",
    "            y = 1\n",
    "        else:\n",
    "            x = self.neg[idx - self.lengths[0]]\n",
    "            y = 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "fc31fd06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:10:13.918974Z",
     "start_time": "2025-07-14T16:10:13.910312Z"
    }
   },
   "source": [
    "class MLPExperts(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_experts):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.fc1 = nn.Linear(d_model, d_ff * num_experts, bias=True)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.d_ff = d_ff\n",
    "    def forward(self, x, expert_idx):\n",
    "        # x: [B*L, d_model], expert_idx: [B*L, k]\n",
    "        all_hidden = self.fc1(x)  # [B*L, d_ff * num_experts]\n",
    "        all_hidden = all_hidden.view(x.size(0), self.num_experts, self.d_ff)  # [B*L, num_experts, d_ff]\n",
    "        out = []\n",
    "        for i in range(expert_idx.size(1)):\n",
    "            idx = expert_idx[:, i]  # [B*L]\n",
    "            hidden = all_hidden[torch.arange(x.size(0)), idx]  # [B*L, d_ff]\n",
    "            hidden = F.gelu(hidden)\n",
    "            out_i = self.fc2(hidden)  # [B*L, d_model]\n",
    "            out.append(out_i)\n",
    "        out = torch.stack(out, dim=1)  # [B*L, k, d_model]\n",
    "        return out\n",
    "class NoisyTopKMoE(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_experts=30, k=2, noisy_std=1.0):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.noisy_std = noisy_std\n",
    "        self.experts = MLPExperts(d_model, d_ff, num_experts)\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, d_model]\n",
    "        B, L, D = x.shape\n",
    "        x_flat = x.reshape(-1, D)  # [B*L, D]\n",
    "        gate_logits = self.gate(x_flat)  # [B*L, num_experts]\n",
    "        # Noisy gating\n",
    "        if self.training and self.noisy_std > 0:\n",
    "            noise = torch.randn_like(gate_logits) * self.noisy_std\n",
    "            gate_logits = gate_logits + noise\n",
    "        gate_scores = F.softmax(gate_logits, dim=-1)  # [B*L, num_experts]\n",
    "\n",
    "          # 稀疏路由：只选top-k\n",
    "        topk_val, topk_idx = torch.topk(gate_scores, self.k, dim=-1)  # [B*L, k]\n",
    "        # 负载均衡损失（新版，防止爆炸）\n",
    "        meangate = gate_scores.mean(dim=0)  # [num_experts]\n",
    "        load_balance_loss = (meangate * meangate).sum() * (self.num_experts ** 2)\n",
    "        # 专家并行输出\n",
    "        expert_outs = self.experts(x_flat, topk_idx)  # [B*L, k, d_model]\n",
    "        topk_val = topk_val / (topk_val.sum(dim=-1, keepdim=True) + 1e-9)\n",
    "        moe_out = (expert_outs * topk_val.unsqueeze(-1)).sum(dim=1)  # [B*L, d_model]\n",
    "        moe_out = moe_out.view(B, L, D)\n",
    "        return moe_out, load_balance_loss"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:10:13.921896Z",
     "start_time": "2025-07-14T16:10:13.919966Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1c909a7e9d7c281a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "21ee1baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:10:13.929437Z",
     "start_time": "2025-07-14T16:10:13.922963Z"
    }
   },
   "source": [
    "class TransformerMoEBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_experts=30, k=3, dropout=0.1, noisy_std=1.0):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.moe = NoisyTopKMoE(d_model, d_ff, num_experts, k, noisy_std)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        moe_out, load_balance_loss = self.moe(x)\n",
    "        x = x + self.dropout(moe_out)\n",
    "        x = self.norm2(x)\n",
    "        return x, load_balance_loss\n",
    "\n",
    "class TransformerMoE(nn.Module):\n",
    "    def __init__(self, d_model=640, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerMoEBlock(d_model, nhead, d_ff, num_experts, k, dropout, noisy_std)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        total_load_balance_loss = 0\n",
    "        for layer in self.layers:\n",
    "            x, lb_loss = layer(x)\n",
    "            total_load_balance_loss += lb_loss\n",
    "        x = x.mean(dim=1)  # 池化\n",
    "        logits = self.classifier(x)\n",
    "        return logits, total_load_balance_loss"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:10:13.934926Z",
     "start_time": "2025-07-14T16:10:13.930534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    pre = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    print(f\"Test ACC: {acc:.4f}, PRE: {pre:.4f}, REC: {rec:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\")\n",
    "    return acc, pre, rec, f1, mcc"
   ],
   "id": "e1476e5594aa401d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "dff51c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:17:31.573336Z",
     "start_time": "2025-07-14T16:10:13.936055Z"
    }
   },
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# 数据路径\n",
    "train_pos = '/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/positive_train.npy'\n",
    "train_neg = '/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_train_embedding_enhanced.npy'\n",
    "\n",
    "train_dataset = ProteinNPYDataset(train_pos, train_neg)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerMoE(\n",
    "    d_model=640, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=3, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = GradScaler()  # 在训练前初始化\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, moe_loss_weight=0.01, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():  # 开启混合精度\n",
    "            logits, lb_loss = model(x)\n",
    "            loss = criterion(logits, y) + moe_loss_weight * lb_loss\n",
    "        scaler.scale(loss).backward()      # 用scaler缩放loss反向传播\n",
    "        scaler.step(optimizer)             # 用scaler.step更新参数\n",
    "        scaler.update()                    # 更新scaler状态\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 初始化scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 训练主循环\n",
    "epochs = 10\n",
    "best_acc = 0\n",
    "best_state = None\n",
    "best_path = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_best.pth\"\n",
    "last_path = \"/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler=scaler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    # 保存最后一次模型权重\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "    print(f\"Last model saved at epoch {epoch+1} ({last_path})\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219485/844058870.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # 在训练前初始化\n",
      "/tmp/ipykernel_219485/844058870.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/103 [00:00<?, ?it/s]/tmp/ipykernel_219485/844058870.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # 开启混合精度\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6447\n",
      "Last model saved at epoch 1 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4228\n",
      "Last model saved at epoch 2 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3999\n",
      "Last model saved at epoch 3 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3914\n",
      "Last model saved at epoch 4 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3869\n",
      "Last model saved at epoch 5 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3860\n",
      "Last model saved at epoch 6 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3669\n",
      "Last model saved at epoch 7 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3699\n",
      "Last model saved at epoch 8 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3542\n",
      "Last model saved at epoch 9 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3421\n",
      "Last model saved at epoch 10 (/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/checkpoints/transformer_moe_last.pth)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:17:31.578747Z",
     "start_time": "2025-07-14T16:17:31.575477Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9dafb0ac1be114ef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "897d1af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:17:42.502008Z",
     "start_time": "2025-07-14T16:17:31.581001Z"
    }
   },
   "source": [
    "test_pos = '/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/positive_test.npy'\n",
    "test_neg = '/exp_data/sjx/star/experiments/qianruxuanze/esm2_150M/data/negative_test.npy'\n",
    "test_dataset = ProteinNPYDataset(test_pos, test_neg)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    all_probs = []  # 存储正类概率用于AUC和AUPRC\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # 正类概率\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "        confusion_matrix, roc_auc_score, average_precision_score\n",
    "    )\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    pre = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "    # 混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    sn = rec  # 敏感性\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0  # 特异性\n",
    "\n",
    "    # AUC和AUPRC\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Test ACC: {acc:.4f}, PRE: {pre:.4f}, REC(Sn): {sn:.4f}, SP: {sp:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}, AUC: {auc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "    return acc, pre, sn, sp, f1, mcc, auc, auprc\n",
    "\n",
    "# 测试\n",
    "eval_model(model, test_loader, device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.8903, PRE: 0.9336, REC(Sn): 0.9108, SP: 0.8399, F1: 0.9220, MCC: 0.7380, AUC: 0.9528, AUPRC: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8903394255874674,\n",
       " 0.9335839598997494,\n",
       " 0.910757946210269,\n",
       " 0.8398791540785498,\n",
       " 0.9220297029702971,\n",
       " 0.7380158184030507,\n",
       " 0.9528065652723096,\n",
       " 0.9795031746111332)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:18:06.759966Z",
     "start_time": "2025-07-14T16:17:42.503443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设模型结构和ProteinNPYDataset已定义，device已设置\n",
    "import torch\n",
    "\n",
    "# 1. 加载模型\n",
    "model = TransformerMoE(\n",
    "    d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=3, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load('/exp_data/sjx/star/main_transformer_moe_weight/best_transformer_moe_last.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2. 加载测试集\n",
    "test_pos = '/exp_data/sjx/star/first_data/ESM-embedding/positive_test_embedding.npy'\n",
    "test_neg = '/exp_data/sjx/star/first_data/ESM-embedding/negative_test_embedding.npy'\n",
    "test_dataset = ProteinNPYDataset(test_pos, test_neg)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# 3. 定义评估函数\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    pre = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    print(f\"Test ACC: {acc:.4f}, PRE: {pre:.4f}, REC: {rec:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\")\n",
    "    return acc, pre, rec, f1, mcc\n",
    "\n",
    "# 4. 测试\n",
    "eval_model(model, test_loader, device)"
   ],
   "id": "176b7cb9a5004cf2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219485/4116043199.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/exp_data/sjx/star/main_transformer_moe_weight/best_transformer_moe_last.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.9225, PRE: 0.9483, REC: 0.9425, F1: 0.9454, MCC: 0.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9225413402959095,\n",
       " 0.948339483394834,\n",
       " 0.9425427872860636,\n",
       " 0.9454322501532803,\n",
       " 0.8120485793877618)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 记录\n",
    "第一次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=10\n",
    "\n",
    "结果 Test ACC: 0.9173, PRE: 0.9199, REC: 0.9682, F1: 0.9434, MCC: 0.7939\n",
    "\n",
    "第二次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=5, num_experts=30, k=2, dropout=0.2, noisy_std=1.0, num_classes=2 epoch=20\n",
    "\n",
    "结果 Test ACC: 0.8973, PRE: 0.9464, REC: 0.9071, F1: 0.9263, MCC: 0.7589\n",
    "\n",
    "第三次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=15\n",
    "结果Test ACC: 0.9121, PRE: 0.9544, REC: 0.9205, F1: 0.9371, MCC: 0.7926\n",
    "\n",
    "第四次参数设置d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=10\n",
    "数据集变成不加gan\n",
    "\n",
    "结果：Test ACC: 0.8808, PRE: 0.9570, REC: 0.8716, F1: 0.9123, MCC: 0.7350\n",
    "\n",
    "第五次参数设置d_model=1152, nhead=8, d_ff=2048, num_layers=3, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=10\n",
    "\n",
    "结果：Test ACC: 0.9017, PRE: 0.9094, REC: 0.9572, F1: 0.9327, MCC: 0.7540\n",
    "\n",
    "第六次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=2, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=10 加上了一个kaming初始化\n",
    "\n",
    "结果：Test ACC: 0.9130, PRE: 0.9367, REC: 0.9413, F1: 0.9390, MCC: 0.7871\n",
    "\n",
    "第七次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=40, k=2, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "\n",
    "结果：Test ACC: 0.8782, PRE: 0.9508, REC: 0.8741, F1: 0.9108, MCC: 0.7260\n",
    "\n",
    "第八次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=35, k=2, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "\n",
    "结果Test ACC: 0.9112, PRE: 0.9409, REC: 0.9340, F1: 0.9374, MCC: 0.7848\n",
    "\n",
    "第九次参数设置d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=25, k=2, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "\n",
    "结果Test ACC: 0.8982, PRE: 0.8899, REC: 0.9780, F1: 0.9319, MCC: 0.7452\n",
    "\n",
    "第十次参数设置 d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=1, dropout=0.1, noisy_std=1.0, num_classes=2 epoch=10\n",
    "\n",
    "结果：Test ACC: 0.9017, PRE: 0.9657, REC: 0.8936, F1: 0.9283, MCC: 0.7786\n",
    "\n",
    "第十一次参数设置d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=3, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "\n",
    "结果Test ACC: 0.9225, PRE: 0.9483, REC: 0.9425, F1: 0.9454, MCC: 0.8120\n",
    "\n",
    "\n",
    "### 十折验证集\n",
    "参数均使用前边的最佳模型的参数\n",
    "\n",
    "结果：========== 10-Fold CV Results ==========\n",
    "Mean ACC: 0.9401 ± 0.0058\n",
    "Mean PRE: 0.9264\n",
    "Mean REC: 0.9260\n",
    "Mean F1:  0.9252\n",
    "Mean MCC: 0.8523\n",
    "\n",
    "### 十折测试集\n",
    "结果：========== 10-Fold Test Results ==========\n",
    "Mean ACC: 0.9155 ± 0.0048\n",
    "Mean PRE: 0.9338\n",
    "Mean REC: 0.9488\n",
    "Mean F1:  0.9411\n",
    "Mean MCC: 0.7925"
   ],
   "id": "20df39fc2248804a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 十折交叉验证",
   "id": "ce8481db9fb0292"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:18:06.781988Z",
     "start_time": "2025-07-14T16:18:06.761366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "import os\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ],
   "id": "d046bb7ba803721d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T16:18:06.789637Z",
     "start_time": "2025-07-14T16:18:06.783181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, moe_loss_weight=0.01, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits, lb_loss = model(x)\n",
    "            loss = criterion(logits, y) + moe_loss_weight * lb_loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    pre = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    print(f\"Val ACC: {acc:.4f}, PRE: {pre:.4f}, REC: {rec:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\")\n",
    "    return acc, pre, rec, f1, mcc"
   ],
   "id": "cc7e459aef30a0bc",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-14T16:18:06.790941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_pos = '/exp_data/sjx/star/first_data/ESM-embedding/positive_train_embedding.npy'\n",
    "train_neg = '/exp_data/sjx/star/gan_data/negative_train_all_combined.npy'\n",
    "\n",
    "# 构建全体索引和标签\n",
    "pos_len = np.load(train_pos, mmap_mode='r').shape[0]\n",
    "neg_len = np.load(train_neg, mmap_mode='r').shape[0]\n",
    "all_indices = np.concatenate([np.arange(pos_len), np.arange(neg_len) + pos_len])\n",
    "all_labels = np.concatenate([np.ones(pos_len, dtype=int), np.zeros(neg_len, dtype=int)])\n",
    "\n",
    "# 数据集\n",
    "full_dataset = ProteinNPYDataset(train_pos, train_neg)\n",
    "\n",
    "# K折分层\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_indices, all_labels), 1):\n",
    "    print(f\"\\n========== Fold {fold}/10 ==========\")\n",
    "    train_loader = DataLoader(Subset(full_dataset, train_idx), batch_size=64, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(Subset(full_dataset, val_idx), batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerMoE(\n",
    "        d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=3, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_state = None\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n[Fold {fold}] Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits, lb_loss = model(x)\n",
    "                loss = criterion(logits, y) + 0.01 * lb_loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        all_preds, all_labels_fold = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits, _ = model(x)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels_fold.extend(y.cpu().numpy())\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "        acc = accuracy_score(all_labels_fold, all_preds)\n",
    "        pre = precision_score(all_labels_fold, all_preds)\n",
    "        rec = recall_score(all_labels_fold, all_preds)\n",
    "        f1 = f1_score(all_labels_fold, all_preds)\n",
    "        mcc = matthews_corrcoef(all_labels_fold, all_preds)\n",
    "        print(f\"Val ACC: {acc:.4f}, PRE: {pre:.4f}, REC: {rec:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_state = model.state_dict()\n",
    "            torch.save(best_state, f\"/exp_data/sjx/star/main_transformer_moe_weight/cv_point/best_fold{fold}.pth\")\n",
    "            print(f\"Best model saved for fold {fold} at epoch {epoch+1}\")\n",
    "\n",
    "    all_metrics.append((best_acc, pre, rec, f1, mcc))\n",
    "    print(f\"[Fold {fold}] Best ACC: {best_acc:.4f}\")\n",
    "\n",
    "# 汇总结果\n",
    "all_metrics = np.array(all_metrics)\n",
    "print(\"\\n========== 10-Fold CV Results ==========\")\n",
    "print(f\"Mean ACC: {all_metrics[:,0].mean():.4f} ± {all_metrics[:,0].std():.4f}\")\n",
    "print(f\"Mean PRE: {all_metrics[:,1].mean():.4f}\")\n",
    "print(f\"Mean REC: {all_metrics[:,2].mean():.4f}\")\n",
    "print(f\"Mean F1:  {all_metrics[:,3].mean():.4f}\")\n",
    "print(f\"Mean MCC: {all_metrics[:,4].mean():.4f}\")"
   ],
   "id": "cde6cd3f45854fd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Fold 1/10 ==========\n",
      "\n",
      "[Fold 1] Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219485/1424473474.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_219485/1424473474.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 十折测试结果",
   "id": "89497dfa96a1f83c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# 测试集\n",
    "test_pos = '/exp_data/sjx/star/first_data/ESM-embedding/positive_test_embedding.npy'\n",
    "test_neg = '/exp_data/sjx/star/first_data/ESM-embedding/negative_test_embedding.npy'\n",
    "test_dataset = ProteinNPYDataset(test_pos, test_neg)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    pre = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    return acc, pre, rec, f1, mcc\n",
    "\n",
    "# 评估每一折\n",
    "all_metrics = []\n",
    "for fold in range(1, 11):\n",
    "    print(f\"\\n========== Test Fold {fold}/10 ==========\")\n",
    "    model = TransformerMoE(\n",
    "        d_model=1152, nhead=8, d_ff=2048, num_layers=4, num_experts=30, k=3, dropout=0.1, noisy_std=1.0, num_classes=2\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(f\"/exp_data/sjx/star/main_transformer_moe_weight/cv_point/best_fold{fold}.pth\", map_location=device))\n",
    "    acc, pre, rec, f1, mcc = eval_model(model, test_loader, device)\n",
    "    print(f\"Test ACC: {acc:.4f}, PRE: {pre:.4f}, REC: {rec:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\")\n",
    "    all_metrics.append((acc, pre, rec, f1, mcc))\n",
    "\n",
    "# 汇总平均\n",
    "all_metrics = np.array(all_metrics)\n",
    "print(\"\\n========== 10-Fold Test Results ==========\")\n",
    "print(f\"Mean ACC: {all_metrics[:,0].mean():.4f} ± {all_metrics[:,0].std():.4f}\")\n",
    "print(f\"Mean PRE: {all_metrics[:,1].mean():.4f}\")\n",
    "print(f\"Mean REC: {all_metrics[:,2].mean():.4f}\")\n",
    "print(f\"Mean F1:  {all_metrics[:,3].mean():.4f}\")\n",
    "print(f\"Mean MCC: {all_metrics[:,4].mean():.4f}\")"
   ],
   "id": "5d6b965da9085033",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
