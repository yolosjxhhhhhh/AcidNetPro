{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:50.762534Z",
     "start_time": "2025-07-22T13:59:50.760772Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1c15f7c7cb09ecdf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T15:03:30.802521Z",
     "start_time": "2025-07-27T15:03:30.530859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 正负样本氨基酸相关性分析\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_fasta(filepath):\n",
    "    seqs = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        seq = ''\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if seq:\n",
    "                    seqs.append(seq)\n",
    "                    seq = ''\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq:\n",
    "            seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "\n",
    "# 合并正负样本\n",
    "pos_train = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/positive_train.fasta')\n",
    "pos_test = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/positive_test.fasta')\n",
    "neg_train = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/negative_train.fasta')\n",
    "neg_test = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/negative_test.fasta')\n",
    "\n",
    "pos_seqs = pos_train + pos_test\n",
    "neg_seqs = neg_train + neg_test\n",
    "\n",
    "# 统计全局频率\n",
    "amino_acids = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "\n",
    "def get_freq(seqs):\n",
    "    total = Counter()\n",
    "    total_len = 0\n",
    "    for seq in seqs:\n",
    "        total.update(seq)\n",
    "        total_len += len(seq)\n",
    "    freq = np.array([total[aa] for aa in amino_acids], dtype=float) / total_len\n",
    "    return freq\n",
    "\n",
    "\n",
    "pos_freq = get_freq(pos_seqs)\n",
    "neg_freq = get_freq(neg_seqs)\n",
    "print(pos_freq)\n",
    "print(neg_freq)"
   ],
   "id": "8383ea620b5fa63c",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.006195Z",
     "start_time": "2025-07-22T13:59:50.954221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lengths = [len(seq) for seq in pos_seqs]\n",
    "print(f\"正样本最大长度: {max(lengths)}, 最小长度: {min(lengths)}, 平均长度: {np.mean(lengths):.2f}\")\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "for i, seq in enumerate(pos_seqs):\n",
    "    for aa in seq:\n",
    "        if aa not in amino_acids:\n",
    "            print(f\"第{i}条序列有异常字符: {aa}\")"
   ],
   "id": "8576615273fe2bcb",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.088461Z",
     "start_time": "2025-07-22T13:59:51.007371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "def filter_valid_sequences(seqs):\n",
    "    return [seq for seq in seqs if all(aa in amino_acids for aa in seq)]\n",
    "pos_seqs_clean = filter_valid_sequences(pos_seqs)\n",
    "neg_seqs_clean = filter_valid_sequences(neg_seqs)\n",
    "print(f\"过滤后正样本数: {len(pos_seqs_clean)}\")\n",
    "print(f\"过滤后负样本数: {len(neg_seqs_clean)}\")"
   ],
   "id": "e44ce9a7cec486a9",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.097526Z",
     "start_time": "2025-07-22T13:59:51.090322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_by_length(seqs, max_len=1000):\n",
    "    return [seq for seq in seqs if len(seq) <= max_len]\n",
    "pos_seqs_final = filter_by_length(pos_seqs_clean, max_len=1000)\n",
    "neg_seqs_final = filter_by_length(neg_seqs_clean, max_len=1000)\n",
    "print(f'最终正样本数: {len(pos_seqs_final)}')\n",
    "print(f'最终负样本数: {len(neg_seqs_final)}')"
   ],
   "id": "af8b74a8fe9801dc",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.251329Z",
     "start_time": "2025-07-22T13:59:51.099255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "num_aa = len(aa_list)\n",
    "aa2idx = {aa: i for i, aa in enumerate(aa_list)}\n",
    "aa_freq_matrix = np.zeros((num_aa, len(pos_seqs_final)))\n",
    "for i, seq_str in enumerate(pos_seqs_final):\n",
    "    if not seq_str:\n",
    "        continue\n",
    "    int_seq = np.array([aa2idx.get(aa, -1) for aa in seq_str], dtype=int)\n",
    "    valid_indices = int_seq[int_seq != -1]\n",
    "    total_len = len(valid_indices)\n",
    "    if total_len == 0:\n",
    "        continue\n",
    "    counts = np.bincount(valid_indices, minlength=num_aa)\n",
    "    aa_freq_matrix[:, i] = counts / total_len\n",
    "\n",
    "print(\"频率矩阵 shape:\", aa_freq_matrix.shape)\n",
    "print(\"全0行数：\", np.sum(aa_freq_matrix.sum(axis=1) == 0))\n",
    "print(\"全0列数：\", np.sum(aa_freq_matrix.sum(axis=0) == 0))\n",
    "print(\"nan数：\", np.isnan(aa_freq_matrix).sum())\n",
    "print(\"inf数：\", np.isinf(aa_freq_matrix).sum())\n",
    "print(\"每行最小值：\", aa_freq_matrix.min(axis=1))\n",
    "print(\"每行最大值：\", aa_freq_matrix.max(axis=1))\n",
    "print(\"每列最小值：\", aa_freq_matrix.min(axis=0)[:10], \"...\")\n",
    "print(\"每列最大值：\", aa_freq_matrix.max(axis=0)[:10], \"...\")"
   ],
   "id": "22a9c696aed191da",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.426670Z",
     "start_time": "2025-07-22T13:59:51.252687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_aa_correlation_matrix_optimized(seqs, amino_acids=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
    "    aa_list = list(amino_acids)\n",
    "    num_aa = len(aa_list)\n",
    "    aa2idx = {aa: i for i, aa in enumerate(aa_list)}\n",
    "    aa_freq_matrix = np.zeros((num_aa, len(seqs)))\n",
    "    for i, seq_str in enumerate(tqdm(seqs, desc=\"统计频率\")):\n",
    "        if not seq_str:\n",
    "            continue\n",
    "        int_seq = np.array([aa2idx.get(aa, -1) for aa in seq_str], dtype=int)\n",
    "        valid_indices = int_seq[int_seq != -1]\n",
    "        total_len = len(valid_indices)\n",
    "        if total_len == 0:\n",
    "            continue\n",
    "        counts = np.bincount(valid_indices, minlength=num_aa)\n",
    "        aa_freq_matrix[:, i] = counts / total_len\n",
    "    print(\"频率矩阵 shape:\", aa_freq_matrix.shape)\n",
    "    print(\"准备调用 np.corrcoef ...\")\n",
    "    corr_matrix = np.corrcoef(aa_freq_matrix)\n",
    "    print(\"np.corrcoef 完成\")\n",
    "    return corr_matrix\n",
    "\n",
    "# --- 使用优化后的函数 ---\n",
    "print(\"计算正样本氨基酸相关性...\")\n",
    "pos_corr = get_aa_correlation_matrix_optimized(pos_seqs_final)\n",
    "print(\"计算负样本氨基酸相关性...\")\n",
    "neg_corr = get_aa_correlation_matrix_optimized(neg_seqs_final)\n",
    "\n",
    "print(f\"正样本序列数: {len(pos_seqs)}\")\n",
    "print(f\"负样本序列数: {len(neg_seqs)}\")\n",
    "print(f\"相关性矩阵形状: {pos_corr.shape}\")\n",
    "print(\"计算完成！\")\n",
    "\n"
   ],
   "id": "d8c7f498b803a7c1",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:51.432067Z",
     "start_time": "2025-07-22T13:59:51.428532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ],
   "id": "b72049945ba2b06c",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:59:52.150390Z",
     "start_time": "2025-07-22T13:59:51.435435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# 假设你已经有 pos_corr, neg_corr, amino_acids\n",
    "pos_corr_df = pd.DataFrame(pos_corr, index=amino_acids, columns=amino_acids)\n",
    "neg_corr_df = pd.DataFrame(neg_corr, index=amino_acids, columns=amino_acids)\n",
    "\n",
    "\n",
    "def plot_corr_beauty(corr_df, title, save_path, highlight_boxes=None):\n",
    "    n = len(corr_df)\n",
    "    plt.figure(figsize=(10, 9))\n",
    "    ax = plt.gca()\n",
    "    # 右上角（含主对角线）：画圆圈，主对角线只显示字母\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if i == j:\n",
    "                # 主对角线：只显示字母\n",
    "                ax.text(j, i, corr_df.columns[i], ha='center', va='center', fontsize=15, fontweight='bold', color='r')\n",
    "            else:\n",
    "                color = plt.cm.coolwarm((val + 0.4) / 0.8)\n",
    "                size = abs(val) * 800 if abs(val) > 0.01 else 0\n",
    "                ax.scatter(j, i, s=size, color=color, alpha=0.8, edgecolor='k', linewidth=0.5)\n",
    "    # 左下角：显示数值\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=10, color='w' if abs(val) > 0.2 else 'gray')\n",
    "    # 高亮部分区域\n",
    "    if highlight_boxes:\n",
    "        for (i0, j0, w, h) in highlight_boxes:\n",
    "            rect = Rectangle((j0 - 0.5, i0 - 0.5), w, h, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=13)\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=13)\n",
    "    ax.set_xlim(-0.5, n - 0.5)\n",
    "    ax.set_ylim(n - 0.5, -0.5)\n",
    "    plt.title(title, fontsize=16)\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=-0.4, vmax=0.4))\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.045, pad=0.03, orientation='horizontal')\n",
    "    cbar.set_label('Correlation', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "save_dir = '/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "highlight_boxes = [(13, 14, 2, 2), (16, 18, 3, 3)]  # 可根据需要调整\n",
    "\n",
    "plot_corr_beauty(pos_corr_df, 'Amino Acid Correlation (Positive, Beauty)',\n",
    "                 os.path.join(save_dir, 'positive_aa_correlation_beauty.svg'), highlight_boxes)\n",
    "plot_corr_beauty(neg_corr_df, 'Amino Acid Correlation (Negative, Beauty)',\n",
    "                 os.path.join(save_dir, 'negative_aa_correlation_beauty.svg'), highlight_boxes)\n",
    "# Cell 1: 导入依赖"
   ],
   "id": "484142b499f0081",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-27T15:03:42.106563Z",
     "start_time": "2025-07-27T15:03:36.383702Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Cell 2: 加载数据\n",
    "attn_weights_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/attn_weights.npy\"\n",
    "gate_scores_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\"\n",
    "topk_idx_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\"\n",
    "token_types_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/test_token_types.npy\"\n",
    "\n",
    "attn_weights = np.load(attn_weights_path)  # (4, 1149, 300, 300)\n",
    "gate_scores = np.load(gate_scores_path)  # (4, 1149, 300, 30)\n",
    "topk_idx = np.load(topk_idx_path)  # (4, 1149, 300, 3)\n",
    "token_types = np.load(token_types_path)  # (1149, 300)\n",
    "\n",
    "print(\"数据形状:\")\n",
    "print(f\"attn_weights: {attn_weights.shape}\")\n",
    "print(f\"gate_scores: {gate_scores.shape}\")\n",
    "print(f\"topk_idx: {topk_idx.shape}\")\n",
    "print(f\"token_types: {token_types.shape}\")\n",
    "\n",
    "# Cell 3: 选择要分析的序列和层\n",
    "seq_idx = 0  # 第一条序列\n",
    "layer_idx = 0  # 第一层\n",
    "seq_len = 140  # 实际序列长度（去除padding）\n",
    "\n",
    "# 获取该序列的数据\n",
    "seq_attn = attn_weights[layer_idx, seq_idx, :seq_len, :seq_len]  # (seq_len, seq_len)\n",
    "seq_gate = gate_scores[layer_idx, seq_idx, :seq_len]  # (seq_len, 30)\n",
    "seq_experts = topk_idx[layer_idx, seq_idx, :seq_len]  # (seq_len, 3)\n",
    "seq_tokens = token_types[seq_idx, :seq_len]  # (seq_len,)\n",
    "\n",
    "print(f\"分析序列 {seq_idx}，层 {layer_idx}，长度 {seq_len}\")\n",
    "\n",
    "\n",
    "# Cell 4: 准备桑基图数据 - Token到Expert的分配\n",
    "def prepare_sankey_data(seq_attn, seq_gate, seq_experts, seq_tokens, top_k=3):\n",
    "    \"\"\"\n",
    "    准备桑基图数据：Token -> Expert -> Attention Weight\n",
    "    \"\"\"\n",
    "    # 节点定义\n",
    "    token_nodes = [f\"Token_{i}\" for i in range(len(seq_tokens))]\n",
    "\n",
    "    expert_nodes = [f\"Expert_{eid}\" for eid in range(30)]\n",
    "\n",
    "    # 边数据\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "    color = []\n",
    "\n",
    "    # Token到Expert的连接\n",
    "    for token_idx in range(len(seq_tokens)):\n",
    "        if seq_tokens[token_idx] == -1:  # 跳过padding\n",
    "            continue\n",
    "\n",
    "        for k in range(top_k):\n",
    "            expert_id = seq_experts[token_idx, k]\n",
    "            gate_score = seq_gate[token_idx, expert_id]\n",
    "\n",
    "            source.append(token_idx)\n",
    "            target.append(len(token_nodes) + expert_id)\n",
    "            value.append(gate_score)\n",
    "            color.append(f\"rgba(100, 149, 237, {gate_score})\")\n",
    "\n",
    "    # Expert到Attention的连接\n",
    "    attention_in = seq_attn.sum(axis=0)  # 每个token被关注的程度\n",
    "\n",
    "    for expert_id in range(30):\n",
    "        # 找出该专家处理的token\n",
    "        expert_tokens = []\n",
    "        for token_idx in range(len(seq_tokens)):\n",
    "            if expert_id in seq_experts[token_idx]:\n",
    "                expert_tokens.append(token_idx)\n",
    "\n",
    "        if expert_tokens:\n",
    "            # 计算该专家处理的token的平均注意力\n",
    "            avg_attention = np.mean([attention_in[t] for t in expert_tokens])\n",
    "\n",
    "            source.append(len(token_nodes) + expert_id)\n",
    "            target.append(len(token_nodes) + 30)  # 虚拟的\"Attention\"节点\n",
    "            value.append(avg_attention)\n",
    "            color.append(f\"rgba(255, 99, 71, {avg_attention})\")\n",
    "\n",
    "    return token_nodes, expert_nodes, source, target, value, color\n",
    "\n",
    "\n",
    "token_nodes, expert_nodes, source, target, value, color = prepare_sankey_data(\n",
    "    seq_attn, seq_gate, seq_experts, seq_tokens\n",
    ")\n",
    "\n",
    "\n",
    "# Cell 5: 创建桑基图\n",
    "def create_sankey_diagram(token_nodes, expert_nodes, source, target, value, color):\n",
    "    \"\"\"\n",
    "    创建桑基图\n",
    "    \"\"\"\n",
    "    # 所有节点\n",
    "    all_nodes = token_nodes + expert_nodes + [\"Attention\"]\n",
    "    node_colors = [\"lightblue\"] * len(token_nodes) + [\"lightgreen\"] * len(expert_nodes) + [\"lightcoral\"]\n",
    "\n",
    "    # 创建桑基图\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=all_nodes,\n",
    "            color=node_colors\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=source,\n",
    "            target=target,\n",
    "            value=value,\n",
    "            color=color\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Token-Expert-Attention Flow (Sequence {seq_idx}, Layer {layer_idx})\",\n",
    "        font_size=10,\n",
    "        height=800,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = create_sankey_diagram(token_nodes, expert_nodes, source, target, value, color)\n",
    "fig.show()\n",
    "# Cell 6: 保存桑基图\n",
    "save_path = f\"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/sankey_seq{seq_idx}_layer{layer_idx}.html\"\n",
    "fig.write_html(save_path)\n",
    "print(f\"桑基图已保存到: {save_path}\")\n",
    "# Cell 1: 导入依赖\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Cell 2: 读取 FASTA 格式的氨基酸序列\n",
    "def read_fasta(filepath):\n",
    "    seqs = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        seq = ''\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if seq:\n",
    "                    seqs.append(seq)\n",
    "                    seq = ''\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq:\n",
    "            seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "\n",
    "# 读取正负样本序列\n",
    "pos_train = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/positive_train.fasta')\n",
    "pos_test = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/positive_test.fasta')\n",
    "neg_train = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/negative_train.fasta')\n",
    "neg_test = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/negative_test.fasta')\n",
    "\n",
    "pos_seqs = pos_train + pos_test\n",
    "neg_seqs = neg_train + neg_test\n",
    "\n",
    "print(f\"正样本序列数: {len(pos_seqs)}\")\n",
    "print(f\"负样本序列数: {len(neg_seqs)}\")\n",
    "print(f\"第一条正样本序列: {pos_seqs[0][:50]}...\")\n",
    "# Cell 3: 设置氨基酸顺序\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "num_aa = len(amino_acids)\n",
    "print(\"氨基酸列表:\", amino_acids)\n",
    "\n",
    "\n",
    "# Cell 4: 计算正样本氨基酸相关性矩阵\n",
    "def get_aa_correlation_matrix(seqs):\n",
    "    \"\"\"\n",
    "    计算20种氨基酸之间的相关性矩阵\n",
    "    seqs: list of strings, 每条序列是氨基酸字符串\n",
    "    \"\"\"\n",
    "    aa_freq_matrix = np.zeros((len(amino_acids), len(seqs)))\n",
    "\n",
    "    for i, seq in enumerate(seqs):\n",
    "        # 只统计真实氨基酸，不统计其他字符\n",
    "        valid_aa = [aa for aa in seq if aa in amino_acids]\n",
    "        total_len = len(valid_aa)\n",
    "        if total_len == 0:\n",
    "            print(f\"Warning: sequence {i} has no valid amino acids!\")\n",
    "            continue\n",
    "        c = Counter(valid_aa)\n",
    "        for j, aa in enumerate(amino_acids):\n",
    "            aa_freq_matrix[j, i] = c[aa] / total_len\n",
    "\n",
    "    corr_matrix = np.corrcoef(aa_freq_matrix)\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "print(\"计算正样本氨基酸相关性...\")\n",
    "pos_corr = get_aa_correlation_matrix(pos_seqs)\n",
    "# Cell 5: 计算负样本氨基酸相关性矩阵\n",
    "print(\"计算负样本氨基酸相关性...\")\n",
    "neg_corr = get_aa_correlation_matrix(neg_seqs)\n",
    "# Cell 6: 检查相关性矩阵\n",
    "print(\"正样本相关性矩阵最大值：\", np.max(pos_corr))\n",
    "print(\"正样本相关性矩阵最小值：\", np.min(pos_corr))\n",
    "print(\"负样本相关性矩阵最大值：\", np.max(neg_corr))\n",
    "print(\"负样本相关性矩阵最小值：\", np.min(neg_corr))\n",
    "# Cell 7: 转换为DataFrame\n",
    "pos_corr_df = pd.DataFrame(pos_corr, index=amino_acids, columns=amino_acids)\n",
    "neg_corr_df = pd.DataFrame(neg_corr, index=amino_acids, columns=amino_acids)\n",
    "\n",
    "\n",
    "# Cell 8: 画图函数\n",
    "def plot_corr_beauty(corr_df, title, save_path, highlight_boxes=None):\n",
    "    n = len(corr_df)\n",
    "    plt.figure(figsize=(10, 9))\n",
    "    ax = plt.gca()\n",
    "    # 右上角（含主对角线）：画圆圈，主对角线只显示字母\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if i == j:\n",
    "                # 主对角线：只显示字母\n",
    "                ax.text(j, i, corr_df.columns[i], ha='center', va='center', fontsize=15, fontweight='bold', color='r')\n",
    "            else:\n",
    "                color = plt.cm.coolwarm((val + 0.4) / 0.8)\n",
    "                size = abs(val) * 800 if abs(val) > 0.01 else 0\n",
    "                ax.scatter(j, i, s=size, color=color, alpha=0.8, edgecolor='k', linewidth=0.5)\n",
    "    # 左下角：显示数值\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=10, color='w' if abs(val) > 0.2 else 'gray')\n",
    "    # 高亮部分区域\n",
    "    if highlight_boxes:\n",
    "        for (i0, j0, w, h) in highlight_boxes:\n",
    "            rect = Rectangle((j0 - 0.5, i0 - 0.5), w, h, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=13)\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=13)\n",
    "    ax.set_xlim(-0.5, n - 0.5)\n",
    "    ax.set_ylim(n - 0.5, -0.5)\n",
    "    plt.title(title, fontsize=16)\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=-0.4, vmax=0.4))\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.045, pad=0.03, orientation='horizontal')\n",
    "    cbar.set_label('Correlation', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T16:36:59.204071Z",
     "start_time": "2025-07-27T16:36:57.525934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "\n",
    "# 加载数据\n",
    "attn_weights_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/attn_weights.npy\"\n",
    "gate_scores_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\"\n",
    "topk_idx_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\"\n",
    "token_types_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/test_token_types.npy\"\n",
    "\n",
    "attn_weights = np.load(attn_weights_path)  # (4, 1149, 300, 300)\n",
    "gate_scores = np.load(gate_scores_path)    # (4, 1149, 300, 30)\n",
    "topk_idx = np.load(topk_idx_path)          # (4, 1149, 300, 3)\n",
    "token_types = np.load(token_types_path)    # (1149, 300)\n",
    "\n",
    "# 配置要分析的层和序列\n",
    "layer_idx = 3     # 分析最后一层，通常最重要\n",
    "seq_idx = 42      # 选择一条代表性序列\n",
    "max_seq_len = 100 # 限制序列长度，使图更清晰\n",
    "k_value = 3       # 每个token使用的专家数\n",
    "\n",
    "# 计算实际序列长度（去除padding）\n",
    "seq_len = min((token_types[seq_idx] != -1).sum(), max_seq_len)\n",
    "\n",
    "# 获取该序列的数据\n",
    "seq_attn = attn_weights[layer_idx, seq_idx, :seq_len, :seq_len]\n",
    "seq_gate = gate_scores[layer_idx, seq_idx, :seq_len]\n",
    "seq_experts = topk_idx[layer_idx, seq_idx, :seq_len]\n",
    "\n",
    "# 分析专家使用情况\n",
    "experts_usage = np.zeros(30)\n",
    "for token_idx in range(seq_len):\n",
    "    for k in range(k_value):\n",
    "        expert_id = seq_experts[token_idx, k]\n",
    "        gate_val = seq_gate[token_idx, expert_id]\n",
    "        experts_usage[expert_id] += gate_val\n",
    "\n",
    "# 选择使用率最高的TOP-10专家\n",
    "top_experts_idx = np.argsort(experts_usage)[-10:][::-1]\n",
    "top_experts_usage = experts_usage[top_experts_idx]\n",
    "\n",
    "# 准备桑基图数据\n",
    "def prepare_advanced_sankey_data():\n",
    "    # 1. 定义节点\n",
    "    # 第一列：氨基酸残基位置（用位置表示）\n",
    "    amino_nodes = [f\"Pos_{i+1}\" for i in range(seq_len)]\n",
    "    \n",
    "    # 第二列：专家组（只包括使用率TOP-10的专家）\n",
    "    expert_nodes = [f\"Expert_{i}\" for i in top_experts_idx]\n",
    "    \n",
    "    # 第三列：主要关注区域（根据attention map聚类）\n",
    "    # 这里我们将序列分为N个区域\n",
    "    n_regions = 5\n",
    "    region_size = seq_len // n_regions\n",
    "    region_nodes = [f\"Region_{i+1}\" for i in range(n_regions)]\n",
    "    \n",
    "    # 2. 计算边\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "    colors = []\n",
    "    \n",
    "    # 位置到专家的连接\n",
    "    for pos in range(seq_len):\n",
    "        for k in range(k_value):\n",
    "            expert_id = seq_experts[pos, k]\n",
    "            # 只关注TOP-10专家\n",
    "            if expert_id in top_experts_idx:\n",
    "                expert_idx = list(top_experts_idx).index(expert_id)\n",
    "                gate_value = seq_gate[pos, expert_id]\n",
    "                \n",
    "                # 只考虑有意义的连接\n",
    "                if gate_value > 0.1:\n",
    "                    sources.append(pos)\n",
    "                    targets.append(seq_len + expert_idx)\n",
    "                    values.append(gate_value * 100)  # 放大比例以使图形更清晰\n",
    "                    colors.append(f\"rgba(86, 180, 233, {min(gate_value, 0.9)})\")\n",
    "    \n",
    "    # 专家到区域的连接\n",
    "    # 计算每个专家对每个区域的贡献\n",
    "    for e_idx, expert_id in enumerate(top_experts_idx):\n",
    "        # 计算该专家处理的token的注意力分布\n",
    "        expert_attention = np.zeros(n_regions)\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            # 检查该位置是否使用了这个专家\n",
    "            if expert_id in seq_experts[pos]:\n",
    "                # 计算该位置关注的区域\n",
    "                for r_idx in range(n_regions):\n",
    "                    start = r_idx * region_size\n",
    "                    end = min((r_idx + 1) * region_size, seq_len)\n",
    "                    # 累加该位置对每个区域的注意力\n",
    "                    region_attn = seq_attn[pos, start:end].sum()\n",
    "                    expert_attention[r_idx] += region_attn\n",
    "        \n",
    "        # 归一化专家到区域的注意力分数\n",
    "        if expert_attention.sum() > 0:\n",
    "            expert_attention = expert_attention / expert_attention.sum()\n",
    "            \n",
    "            # 添加专家到区域的连接\n",
    "            for r_idx, attn in enumerate(expert_attention):\n",
    "                if attn > 0.05:  # 过滤小值\n",
    "                    sources.append(seq_len + e_idx)\n",
    "                    targets.append(seq_len + len(top_experts_idx) + r_idx)\n",
    "                    values.append(attn * 200)  # 放大注意力值\n",
    "                    colors.append(f\"rgba(230, 159, 0, {min(attn * 2, 0.9)})\")\n",
    "    \n",
    "    # 3. 节点颜色和标签\n",
    "    node_colors = []\n",
    "    node_labels = []\n",
    "    \n",
    "    # 氨基酸位置节点\n",
    "    for i in range(seq_len):\n",
    "        node_colors.append(\"rgba(86, 180, 233, 0.8)\")\n",
    "        if i % 10 == 0:  # 只在每10个位置显示标签，避免拥挤\n",
    "            node_labels.append(f\"Pos {i+1}\")\n",
    "        else:\n",
    "            node_labels.append(\"\")\n",
    "    \n",
    "    # 专家节点\n",
    "    for expert_id in top_experts_idx:\n",
    "        node_colors.append(\"rgba(0, 158, 115, 0.8)\")\n",
    "        node_labels.append(f\"Expert {expert_id}\")\n",
    "    \n",
    "    # 区域节点\n",
    "    for i in range(n_regions):\n",
    "        node_colors.append(\"rgba(240, 228, 66, 0.8)\")\n",
    "        start = i * region_size + 1\n",
    "        end = min((i + 1) * region_size, seq_len)\n",
    "        node_labels.append(f\"Region {i+1}\\n({start}-{end})\")\n",
    "    \n",
    "    all_nodes = amino_nodes + expert_nodes + region_nodes\n",
    "    \n",
    "    return node_labels, node_colors, sources, targets, values, colors\n",
    "\n",
    "# 生成桑基图数据\n",
    "node_labels, node_colors, sources, targets, values, colors = prepare_advanced_sankey_data()\n",
    "\n",
    "# 创建桑基图\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=node_labels,\n",
    "        color=node_colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values,\n",
    "        color=colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "# 设置图表标题和样式\n",
    "fig.update_layout(\n",
    "    title_text=f\"MoE Expert Flow Analysis: Layer {layer_idx+1}, Sequence {seq_idx}\",\n",
    "    font_size=12,\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# 保存图表\n",
    "save_dir = \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"advanced_sankey_layer{layer_idx+1}_seq{seq_idx}.html\")\n",
    "fig.write_html(save_path)\n",
    "\n",
    "# 添加可视化专家使用情况的柱状图\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(top_experts_idx)), top_experts_usage, color='skyblue')\n",
    "\n",
    "# 为每个柱状图添加标签\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f\"Expert {top_experts_idx[i]}\", ha='center', rotation=45)\n",
    "\n",
    "plt.xlabel(\"Top Experts\", fontsize=14)\n",
    "plt.ylabel(\"Usage Score\", fontsize=14)\n",
    "plt.title(f\"Top-10 Expert Usage (Layer {layer_idx+1}, Sequence {seq_idx})\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存柱状图\n",
    "expert_usage_path = os.path.join(save_dir, f\"expert_usage_layer{layer_idx+1}_seq{seq_idx}.svg\")\n",
    "plt.savefig(expert_usage_path, format=\"svg\", bbox_inches=\"tight\")\n",
    "\n",
    "print(f\"桑基图已保存到: {save_path}\")\n",
    "print(f\"专家使用情况图已保存到: {expert_usage_path}\")"
   ],
   "id": "c97b5457f4ce260c",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T16:47:25.438996Z",
     "start_time": "2025-07-27T16:47:24.526912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 加载数据\n",
    "attn_weights_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/attn_weights.npy\"\n",
    "gate_scores_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\"\n",
    "topk_idx_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\"\n",
    "token_types_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/test_token_types.npy\"\n",
    "\n",
    "attn_weights = np.load(attn_weights_path)  # (4, 1149, 300, 300)\n",
    "gate_scores = np.load(gate_scores_path)    # (4, 1149, 300, 30)\n",
    "topk_idx = np.load(topk_idx_path)          # (4, 1149, 300, 3)\n",
    "token_types = np.load(token_types_path)    # (1149, 300)\n",
    "\n",
    "# 配置要分析的层和序列 - 使用第一条序列\n",
    "layer_idx = 3     # 分析最后一层\n",
    "seq_idx = 0       # 第一条序列\n",
    "seq_len = 140     # 第一条序列的长度\n",
    "k_value = 3       # 每个token使用的专家数\n",
    "\n",
    "print(f\"分析序列 {seq_idx} (层 {layer_idx+1})，长度 {seq_len}\")\n",
    "\n",
    "# 获取该序列的数据\n",
    "seq_attn = attn_weights[layer_idx, seq_idx, :seq_len, :seq_len]\n",
    "seq_gate = gate_scores[layer_idx, seq_idx, :seq_len]\n",
    "seq_experts = topk_idx[layer_idx, seq_idx, :seq_len]\n",
    "\n",
    "# 分析专家使用情况 - 修正计算方式\n",
    "experts_usage = np.zeros(30)\n",
    "for token_idx in range(seq_len):\n",
    "    for k in range(k_value):\n",
    "        expert_id = seq_experts[token_idx, k]\n",
    "        gate_val = seq_gate[token_idx, expert_id]\n",
    "        experts_usage[expert_id] += gate_val\n",
    "\n",
    "# 选择使用率最高的TOP-8专家\n",
    "top_experts_idx = np.argsort(experts_usage)[-8:][::-1]\n",
    "top_experts_usage = experts_usage[top_experts_idx]\n",
    "print(f\"使用率最高的8个专家: {top_experts_idx}\")\n",
    "\n",
    "# 准备桑基图数据 - 修正版\n",
    "def prepare_enhanced_sankey_data():\n",
    "    # 1. 定义节点分组\n",
    "    n_pos_groups = 8  # 将140个位置分为8组\n",
    "    pos_group_size = seq_len // n_pos_groups\n",
    "    \n",
    "    # 位置组节点标签\n",
    "    pos_group_nodes = [f\"Pos {g*pos_group_size+1}-{min((g+1)*pos_group_size, seq_len)}\" \n",
    "                      for g in range(n_pos_groups)]\n",
    "    \n",
    "    # 专家节点标签\n",
    "    expert_nodes = [f\"Expert {i}\" for i in top_experts_idx]\n",
    "    \n",
    "    # 区域节点标签\n",
    "    n_regions = 4  # 4个区域更易于可视化\n",
    "    region_size = seq_len // n_regions\n",
    "    region_nodes = [f\"Region {i+1}\\n({i*region_size+1}-{min((i+1)*region_size, seq_len)})\" \n",
    "                   for i in range(n_regions)]\n",
    "    \n",
    "    # 2. 初始化权重矩阵\n",
    "    pos_to_expert_weight = np.zeros((n_pos_groups, len(top_experts_idx)))\n",
    "    expert_to_region_weight = np.zeros((len(top_experts_idx), n_regions))\n",
    "    \n",
    "    # 3. 计算位置组到专家的权重\n",
    "    for pos in range(seq_len):\n",
    "        pos_group = min(pos // pos_group_size, n_pos_groups - 1)\n",
    "        \n",
    "        for k in range(k_value):\n",
    "            expert_id = seq_experts[pos, k]\n",
    "            if expert_id in top_experts_idx:\n",
    "                expert_idx = list(top_experts_idx).index(expert_id)\n",
    "                gate_value = seq_gate[pos, expert_id]\n",
    "                pos_to_expert_weight[pos_group, expert_idx] += gate_value\n",
    "    \n",
    "    # 4. 计算专家到区域的权重\n",
    "    for pos in range(seq_len):\n",
    "        # 计算该位置的注意力分布\n",
    "        attn_dist = seq_attn[pos]\n",
    "        \n",
    "        for k in range(k_value):\n",
    "            expert_id = seq_experts[pos, k]\n",
    "            if expert_id in top_experts_idx:\n",
    "                expert_idx = list(top_experts_idx).index(expert_id)\n",
    "                gate_value = seq_gate[pos, expert_id]\n",
    "                \n",
    "                # 计算该专家关注的区域\n",
    "                for r_idx in range(n_regions):\n",
    "                    start = r_idx * region_size\n",
    "                    end = min((r_idx + 1) * region_size, seq_len)\n",
    "                    region_attn = attn_dist[start:end].sum() * gate_value\n",
    "                    expert_to_region_weight[expert_idx, r_idx] += region_attn\n",
    "    \n",
    "    # 5. 对每个专家的区域权重进行归一化\n",
    "    for expert_idx in range(len(top_experts_idx)):\n",
    "        if expert_to_region_weight[expert_idx].sum() > 0:\n",
    "            expert_to_region_weight[expert_idx] = expert_to_region_weight[expert_idx] / expert_to_region_weight[expert_idx].sum()\n",
    "    \n",
    "    # 6. 构建桑基图链接数据\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "    colors = []\n",
    "    \n",
    "    # 添加位置到专家的边\n",
    "    for pos_group in range(n_pos_groups):\n",
    "        for expert_idx in range(len(top_experts_idx)):\n",
    "            weight = pos_to_expert_weight[pos_group, expert_idx]\n",
    "            if weight > 0.05:  # 过滤微小连接\n",
    "                sources.append(pos_group)\n",
    "                targets.append(n_pos_groups + expert_idx)\n",
    "                values.append(weight * 15)  # 放大权重使图形更清晰\n",
    "                colors.append(f\"rgba(86, 180, 233, {min(weight*1.5, 0.9)})\")\n",
    "    \n",
    "    # 添加专家到区域的边\n",
    "    for expert_idx in range(len(top_experts_idx)):\n",
    "        for r_idx in range(n_regions):\n",
    "            weight = expert_to_region_weight[expert_idx, r_idx]\n",
    "            if weight > 0.05:  # 过滤微小连接\n",
    "                sources.append(n_pos_groups + expert_idx)\n",
    "                targets.append(n_pos_groups + len(top_experts_idx) + r_idx)\n",
    "                values.append(weight * 20)  # 放大权重\n",
    "                colors.append(f\"rgba(230, 159, 0, {min(weight*1.5, 0.9)})\")\n",
    "    \n",
    "    # 7. 设置节点颜色\n",
    "    pos_colors = [\"rgba(86, 180, 233, 0.9)\"] * n_pos_groups  # 蓝色\n",
    "    expert_colors = [\"rgba(0, 158, 115, 0.9)\"] * len(top_experts_idx)  # 绿色\n",
    "    region_colors = [\"rgba(240, 228, 66, 0.9)\"] * n_regions  # 黄色\n",
    "    \n",
    "    all_node_colors = pos_colors + expert_colors + region_colors\n",
    "    all_nodes = pos_group_nodes + expert_nodes + region_nodes\n",
    "    \n",
    "    return all_nodes, all_node_colors, sources, targets, values, colors\n",
    "\n",
    "# 生成桑基图数据\n",
    "nodes, node_colors, sources, targets, values, colors = prepare_enhanced_sankey_data()\n",
    "\n",
    "# 创建桑基图\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        color=node_colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values,\n",
    "        color=colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "# 设置图表标题和样式\n",
    "fig.update_layout(\n",
    "    title_text=f\"MoE三级流动分析: 位置组 → 专家 → 区域<br>层 {layer_idx+1}, 序列 {seq_idx} (长度 {seq_len})\",\n",
    "    font=dict(size=14, family=\"Arial\"),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    ")\n",
    "\n",
    "# 保存图表\n",
    "save_dir = \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"three_level_sankey_seq{seq_idx}_layer{layer_idx+1}.html\")\n",
    "fig.write_html(save_path)\n",
    "\n",
    "print(f\"三级桑基图已保存到: {save_path}\")"
   ],
   "id": "586633c04c8b86f3",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T17:14:02.057817Z",
     "start_time": "2025-07-27T16:59:47.708199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/info_flow\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 加载数据\n",
    "attn_weights_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/attn_weights.npy\"\n",
    "gate_scores_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\"\n",
    "topk_idx_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\"\n",
    "token_types_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/test_token_types.npy\"\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "attn_weights = np.load(attn_weights_path)  # (4, 1149, 300, 300)\n",
    "gate_scores = np.load(gate_scores_path)    # (4, 1149, 300, 30)\n",
    "topk_idx = np.load(topk_idx_path)          # (4, 1149, 300, 3)\n",
    "token_types = np.load(token_types_path)    # (1149, 300)\n",
    "\n",
    "num_layers, num_seqs, seq_len, _ = attn_weights.shape\n",
    "num_experts = gate_scores.shape[-1]\n",
    "topk = topk_idx.shape[-1]\n",
    "\n",
    "print(f\"数据加载完成，形状: layers={num_layers}, seqs={num_seqs}, len={seq_len}, experts={num_experts}\")\n",
    "\n",
    "# 1. 计算每个专家的注意力模式\n",
    "def analyze_expert_attention_patterns(layer_idx=3):\n",
    "    \"\"\"分析每个专家的注意力模式\"\"\"\n",
    "    print(f\"分析层 {layer_idx+1} 的专家注意力模式...\")\n",
    "    \n",
    "    # 初始化累积注意力和计数\n",
    "    expert_attention_sum = np.zeros((num_experts, seq_len, seq_len))\n",
    "    expert_token_count = np.zeros(num_experts)\n",
    "    \n",
    "    # 收集每个专家处理的token的注意力模式\n",
    "    for seq_idx in range(num_seqs):\n",
    "        # 确定该序列的有效长度\n",
    "        valid_len = min(seq_len, (token_types[seq_idx] != -1).sum())\n",
    "        if valid_len == 0:\n",
    "            continue\n",
    "            \n",
    "        seq_attn = attn_weights[layer_idx, seq_idx, :valid_len, :valid_len]\n",
    "        seq_experts = topk_idx[layer_idx, seq_idx, :valid_len]\n",
    "        seq_gate = gate_scores[layer_idx, seq_idx, :valid_len]\n",
    "        \n",
    "        # 为每个位置累加注意力权重\n",
    "        for pos in range(valid_len):\n",
    "            for k in range(topk):\n",
    "                expert_id = seq_experts[pos, k]\n",
    "                gate_value = seq_gate[pos, expert_id]\n",
    "                \n",
    "                # 加权累加注意力\n",
    "                attn_pattern = seq_attn[pos, :valid_len]\n",
    "                expert_attention_sum[expert_id, :valid_len, :valid_len] += gate_value * np.outer(\n",
    "                    np.ones(valid_len), attn_pattern)\n",
    "                expert_token_count[expert_id] += gate_value\n",
    "    \n",
    "    # 归一化注意力模式\n",
    "    expert_attention_patterns = np.zeros_like(expert_attention_sum)\n",
    "    for eid in range(num_experts):\n",
    "        if expert_token_count[eid] > 0:\n",
    "            expert_attention_patterns[eid] = expert_attention_sum[eid] / expert_token_count[eid]\n",
    "    \n",
    "    return expert_attention_patterns\n",
    "\n",
    "# 2. 可视化专家注意力模式\n",
    "def visualize_expert_attention_patterns(expert_patterns, layer_idx=3, top_n=10):\n",
    "    \"\"\"可视化专家注意力模式，只展示top_n个专家\"\"\"\n",
    "    print(\"可视化专家注意力模式...\")\n",
    "    \n",
    "    # 计算每个专家的平均注意力强度\n",
    "    expert_attn_strength = np.array([pattern.mean() for pattern in expert_patterns])\n",
    "    top_experts = np.argsort(expert_attn_strength)[-top_n:]\n",
    "    \n",
    "    # 创建自定义colormap\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'custom_cmap', \n",
    "        [(0, 'white'), (0.3, '#ffffcc'), (0.6, '#a1dab4'), (0.8, '#41b6c4'), (1, '#225ea8')],\n",
    "        N=256\n",
    "    )\n",
    "    \n",
    "    # 可视化每个专家的平均注意力模式\n",
    "    n_cols = 5\n",
    "    n_rows = (top_n + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 3.5*n_rows))\n",
    "    \n",
    "    # 将axes展平\n",
    "    if n_rows > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes] if top_n == 1 else axes\n",
    "    \n",
    "    for i, eid in enumerate(top_experts):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # 获取100x100的注意力矩阵\n",
    "            attn_pattern = expert_patterns[eid, :100, :100]\n",
    "            \n",
    "            # 绘制热图\n",
    "            im = ax.imshow(attn_pattern, aspect='equal', \n",
    "                        interpolation='none', cmap=cmap, vmin=0, vmax=attn_pattern.max())\n",
    "            \n",
    "            # 设置标题和标签\n",
    "            strength = expert_attn_strength[eid]\n",
    "            ax.set_title(f\"Expert {eid}\\nStrength: {strength:.3f}\", fontsize=11)\n",
    "            \n",
    "            # 隐藏刻度\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # 添加坐标轴标签\n",
    "            if i >= len(top_experts) - n_cols:  # 最后一行\n",
    "                ax.set_xlabel(\"Query Position\", fontsize=9)\n",
    "            if i % n_cols == 0:  # 第一列\n",
    "                ax.set_ylabel(\"Key Position\", fontsize=9)\n",
    "    \n",
    "    # 隐藏多余的subplot\n",
    "    for i in range(top_n, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # 添加colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.set_label(\"Attention Weight\", fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f\"Layer {layer_idx+1} Top {top_n} Expert Attention Patterns\", \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, right=0.9)\n",
    "    \n",
    "    # 保存图像\n",
    "    fig_path = os.path.join(save_dir, f\"expert_attention_patterns_layer{layer_idx+1}_top{top_n}.pdf\")\n",
    "    plt.savefig(fig_path, format='pdf', bbox_inches='tight')\n",
    "    print(f\"图像已保存至: {fig_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# 3. 分析专家信息流向\n",
    "def analyze_expert_information_flow(layer_idx=3, n_clusters=5):\n",
    "    \"\"\"分析专家处理的token的信息流向\"\"\"\n",
    "    print(f\"分析层 {layer_idx+1} 的信息流向...\")\n",
    "    \n",
    "    # 创建边界位置列表，用于区分序列区域\n",
    "    seq_region_boundaries = np.linspace(0, seq_len, n_clusters+1, dtype=int)\n",
    "    regions = [(seq_region_boundaries[i], seq_region_boundaries[i+1]) \n",
    "               for i in range(len(seq_region_boundaries)-1)]\n",
    "    \n",
    "    # 初始化统计矩阵: expert -> (from_region, to_region)\n",
    "    expert_region_flow = np.zeros((num_experts, n_clusters, n_clusters))\n",
    "    expert_token_count = np.zeros(num_experts)\n",
    "    \n",
    "    # 收集每个专家处理的token的信息流向\n",
    "    for seq_idx in range(num_seqs):\n",
    "        # 确定该序列的有效长度\n",
    "        valid_len = min(seq_len, (token_types[seq_idx] != -1).sum())\n",
    "        if valid_len == 0:\n",
    "            continue\n",
    "            \n",
    "        seq_attn = attn_weights[layer_idx, seq_idx, :valid_len, :valid_len]\n",
    "        seq_experts = topk_idx[layer_idx, seq_idx, :valid_len]\n",
    "        seq_gate = gate_scores[layer_idx, seq_idx, :valid_len]\n",
    "        \n",
    "        # 确定每个位置的区域索引\n",
    "        pos_to_region = np.zeros(valid_len, dtype=int)\n",
    "        for r_idx, (start, end) in enumerate(regions):\n",
    "            mask = (start <= np.arange(valid_len)) & (np.arange(valid_len) < min(end, valid_len))\n",
    "            pos_to_region[mask] = r_idx\n",
    "        \n",
    "        # 为每个专家收集流向数据\n",
    "        for pos in range(valid_len):\n",
    "            from_region = pos_to_region[pos]\n",
    "            \n",
    "            # 计算该位置的注意力分布\n",
    "            attn_dist = seq_attn[pos, :valid_len]\n",
    "            \n",
    "            # 计算对每个区域的注意力总和\n",
    "            region_attn = np.zeros(n_clusters)\n",
    "            for r_idx in range(n_clusters):\n",
    "                start, end = regions[r_idx]\n",
    "                end = min(end, valid_len)\n",
    "                if start < end:\n",
    "                    region_attn[r_idx] = attn_dist[start:end].sum()\n",
    "            \n",
    "            # 归一化区域注意力\n",
    "            region_attn = region_attn / (region_attn.sum() + 1e-10)\n",
    "            \n",
    "            # 更新每个专家的流向统计\n",
    "            for k in range(topk):\n",
    "                expert_id = seq_experts[pos, k]\n",
    "                gate_value = seq_gate[pos, expert_id]\n",
    "                \n",
    "                # 累加从from_region到各个区域的注意力分布\n",
    "                expert_region_flow[expert_id, from_region, :] += gate_value * region_attn\n",
    "                expert_token_count[expert_id] += gate_value\n",
    "    \n",
    "    # 归一化每个专家的流向数据\n",
    "    for eid in range(num_experts):\n",
    "        if expert_token_count[eid] > 0:\n",
    "            # 对每个源区域归一化\n",
    "            for from_region in range(n_clusters):\n",
    "                total = expert_region_flow[eid, from_region].sum()\n",
    "                if total > 0:\n",
    "                    expert_region_flow[eid, from_region] /= total\n",
    "    \n",
    "    return expert_region_flow, regions\n",
    "\n",
    "# 4. 可视化专家信息流向\n",
    "def visualize_expert_information_flow(expert_flow, regions, layer_idx=3, top_n=8):\n",
    "    \"\"\"可视化专家信息流向，只展示top_n个专家\"\"\"\n",
    "    print(\"可视化专家信息流向...\")\n",
    "    \n",
    "    # 计算每个专家的信息流动强度\n",
    "    expert_flow_strength = np.array([flow.sum() for flow in expert_flow])\n",
    "    top_experts = np.argsort(expert_flow_strength)[-top_n:]\n",
    "    \n",
    "    n_clusters = expert_flow.shape[1]\n",
    "    n_cols = min(4, top_n)\n",
    "    n_rows = (top_n + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3.5*n_rows))\n",
    "    \n",
    "    # 将axes展平\n",
    "    if n_rows > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes] if top_n == 1 else axes\n",
    "    \n",
    "    # 创建区域标签\n",
    "    region_labels = [f\"Region {i+1}\\n({start}-{end-1})\" for i, (start, end) in enumerate(regions)]\n",
    "    \n",
    "    for i, eid in enumerate(top_experts):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # 绘制热图\n",
    "            flow_matrix = expert_flow[eid]\n",
    "            sns.heatmap(flow_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", \n",
    "                        xticklabels=region_labels, yticklabels=region_labels,\n",
    "                        ax=ax, cbar=False)\n",
    "            \n",
    "            # 设置标题\n",
    "            ax.set_title(f\"Expert {eid}\", fontsize=12)\n",
    "            \n",
    "            # 设置标签\n",
    "            if i >= len(top_experts) - n_cols:  # 最后一行\n",
    "                ax.set_xlabel(\"To Region\", fontsize=11)\n",
    "            if i % n_cols == 0:  # 第一列\n",
    "                ax.set_ylabel(\"From Region\", fontsize=11)\n",
    "    \n",
    "    # 隐藏多余的subplot\n",
    "    for i in range(top_n, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # 添加一个共享的colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"YlGnBu\")\n",
    "    sm.set_array([0, 1])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label(\"Normalized Flow Strength\", fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f\"Layer {layer_idx+1} Top {top_n} Expert Information Flow\", \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, right=0.9)\n",
    "    \n",
    "    # 保存图像\n",
    "    fig_path = os.path.join(save_dir, f\"expert_information_flow_layer{layer_idx+1}_top{top_n}.pdf\")\n",
    "    plt.savefig(fig_path, format='pdf', bbox_inches='tight')\n",
    "    print(f\"图像已保存至: {fig_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# 5. 分析专家协作模式\n",
    "def analyze_expert_cooperation(layer_idx=3):\n",
    "    \"\"\"分析专家之间的协作模式\"\"\"\n",
    "    print(f\"分析层 {layer_idx+1} 的专家协作模式...\")\n",
    "    \n",
    "    # 初始化协作矩阵\n",
    "    expert_coop = np.zeros((num_experts, num_experts))\n",
    "    \n",
    "    # 统计每对专家共同处理同一位置的次数\n",
    "    for seq_idx in range(num_seqs):\n",
    "        valid_len = min(seq_len, (token_types[seq_idx] != -1).sum())\n",
    "        if valid_len == 0:\n",
    "            continue\n",
    "            \n",
    "        seq_experts = topk_idx[layer_idx, seq_idx, :valid_len]\n",
    "        seq_gate = gate_scores[layer_idx, seq_idx, :valid_len]\n",
    "        \n",
    "        # 对每个位置，统计专家协作\n",
    "        for pos in range(valid_len):\n",
    "            # 获取该位置使用的专家及其门控分数\n",
    "            pos_experts = seq_experts[pos]\n",
    "            pos_gates = np.array([seq_gate[pos, eid] for eid in pos_experts])\n",
    "            \n",
    "            # 更新协作矩阵\n",
    "            for i, e1 in enumerate(pos_experts):\n",
    "                for j, e2 in enumerate(pos_experts):\n",
    "                    if i != j:\n",
    "                        # 使用几何平均作为协作强度\n",
    "                        coop_score = np.sqrt(pos_gates[i] * pos_gates[j])\n",
    "                        expert_coop[e1, e2] += coop_score\n",
    "    \n",
    "    # 归一化协作矩阵\n",
    "    expert_coop_norm = expert_coop / (np.sum(expert_coop) + 1e-10)\n",
    "    \n",
    "    return expert_coop_norm\n",
    "\n",
    "# 6. 可视化专家协作网络\n",
    "def visualize_expert_cooperation(expert_coop, layer_idx=3, threshold=0.01):\n",
    "    \"\"\"可视化专家协作网络\"\"\"\n",
    "    print(\"可视化专家协作网络...\")\n",
    "    \n",
    "    # 创建协作数据框\n",
    "    links = []\n",
    "    for i in range(num_experts):\n",
    "        for j in range(i+1, num_experts):  # 避免重复边\n",
    "            if expert_coop[i, j] > threshold:\n",
    "                links.append({\n",
    "                    'source': f\"Expert {i}\",\n",
    "                    'target': f\"Expert {j}\",\n",
    "                    'weight': expert_coop[i, j]\n",
    "                })\n",
    "    \n",
    "    df_links = pd.DataFrame(links)\n",
    "    \n",
    "    # 如果没有links，增加一个哑边\n",
    "    if len(df_links) == 0:\n",
    "        print(\"警告：专家协作网络中没有超过阈值的连接\")\n",
    "        df_links = pd.DataFrame([{'source': 'Expert 0', 'target': 'Expert 1', 'weight': 0}])\n",
    "    \n",
    "    # 绘制网络图\n",
    "    try:\n",
    "        import networkx as nx\n",
    "        \n",
    "        # 创建图\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # 添加节点\n",
    "        for i in range(num_experts):\n",
    "            G.add_node(f\"Expert {i}\")\n",
    "        \n",
    "        # 添加边\n",
    "        for _, row in df_links.iterrows():\n",
    "            G.add_edge(row['source'], row['target'], weight=row['weight'])\n",
    "        \n",
    "        # 计算网络布局\n",
    "        pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "        \n",
    "        # 获取边权重\n",
    "        edge_weights = [G[u][v]['weight'] * 2000 for u, v in G.edges()]\n",
    "        \n",
    "        # 节点大小基于度中心性\n",
    "        node_size = [300 * (1 + nx.degree_centrality(G)[node]) for node in G.nodes()]\n",
    "        \n",
    "        # 创建图形\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # 绘制边\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, width=edge_weights, alpha=0.7, \n",
    "            edge_color=edge_weights, edge_cmap=plt.cm.Blues\n",
    "        )\n",
    "        \n",
    "        # 绘制节点\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, node_size=node_size, \n",
    "            node_color=[list(nx.degree_centrality(G).values())],\n",
    "            cmap=plt.cm.viridis, alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # 添加标签\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "        \n",
    "        plt.title(f\"Layer {layer_idx+1} Expert Cooperation Network\", fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 保存图像\n",
    "        fig_path = os.path.join(save_dir, f\"expert_cooperation_network_layer{layer_idx+1}.pdf\")\n",
    "        plt.savefig(fig_path, format='pdf', bbox_inches='tight')\n",
    "        print(f\"图像已保存至: {fig_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"警告：需要安装networkx库以绘制网络图\")\n",
    "\n",
    "# 7. 分析注意力区域特化\n",
    "def analyze_attention_specialization(expert_patterns, layer_idx=3):\n",
    "    \"\"\"分析专家的注意力区域特化\"\"\"\n",
    "    print(f\"分析层 {layer_idx+1} 的注意力区域特化...\")\n",
    "    \n",
    "    n = min(100, seq_len)  # 使用前100个位置\n",
    "    \n",
    "    # 定义区域：对角线区域(局部)、远距离区域(全局)、前(N端)、后(C端)\n",
    "    regions = {\n",
    "        'Diagonal': lambda i, j: abs(i-j) <= 5,\n",
    "        'Long Range': lambda i, j: abs(i-j) > n//2,\n",
    "        'N-terminal': lambda i, j: i < n//4 and j < n//4,\n",
    "        'C-terminal': lambda i, j: i >= 3*n//4 and j >= 3*n//4,\n",
    "        'N-to-C': lambda i, j: i < n//4 and j >= 3*n//4,\n",
    "        'C-to-N': lambda i, j: i >= 3*n//4 and j < n//4\n",
    "    }\n",
    "    \n",
    "    # 计算每个专家在各个区域的注意力强度\n",
    "    expert_region_attention = np.zeros((num_experts, len(regions)))\n",
    "    \n",
    "    for eid in range(num_experts):\n",
    "        pattern = expert_patterns[eid, :n, :n]\n",
    "        \n",
    "        for r_idx, (region_name, region_func) in enumerate(regions.items()):\n",
    "            # 创建区域掩码\n",
    "            mask = np.zeros((n, n), dtype=bool)\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    mask[i, j] = region_func(i, j)\n",
    "            \n",
    "            # 计算区域注意力强度\n",
    "            if mask.sum() > 0:\n",
    "                expert_region_attention[eid, r_idx] = pattern[mask].mean() / pattern.mean()\n",
    "    \n",
    "    # 创建数据框\n",
    "    region_names = list(regions.keys())\n",
    "    df_region_attn = pd.DataFrame(expert_region_attention, columns=region_names)\n",
    "    df_region_attn.index.name = 'Expert ID'\n",
    "    \n",
    "    return df_region_attn\n",
    "\n",
    "# 8. 可视化注意力区域特化\n",
    "def visualize_attention_specialization(df_region_attn, layer_idx=3, top_n=10):\n",
    "    \"\"\"可视化专家的注意力区域特化\"\"\"\n",
    "    print(\"可视化注意力区域特化...\")\n",
    "    \n",
    "    # 选择区域特化最强的专家\n",
    "    specialization_score = df_region_attn.max(axis=1) - df_region_attn.min(axis=1)\n",
    "    top_experts = specialization_score.nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # 创建热图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        df_region_attn.loc[top_experts], annot=True, fmt=\".2f\", \n",
    "        cmap=\"YlOrRd\", linewidths=0.5, vmin=0, vmax=df_region_attn.values.max()\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Layer {layer_idx+1} Expert Attention Region Specialization\", fontsize=15)\n",
    "    plt.ylabel(\"Expert ID\", fontsize=12)\n",
    "    plt.xlabel(\"Attention Region\", fontsize=12)\n",
    "    \n",
    "    # 保存图像\n",
    "    fig_path = os.path.join(save_dir, f\"expert_attention_specialization_layer{layer_idx+1}.pdf\")\n",
    "    plt.savefig(fig_path, format='pdf', bbox_inches='tight')\n",
    "    print(f\"图像已保存至: {fig_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# 执行分析\n",
    "for layer_idx in range(num_layers):\n",
    "    # 1-2. 分析和可视化专家注意力模式\n",
    "    expert_patterns = analyze_expert_attention_patterns(layer_idx)\n",
    "    visualize_expert_attention_patterns(expert_patterns, layer_idx)\n",
    "    \n",
    "    # 3-4. 分析和可视化专家信息流向\n",
    "    expert_flow, regions = analyze_expert_information_flow(layer_idx)\n",
    "    visualize_expert_information_flow(expert_flow, regions, layer_idx)\n",
    "    \n",
    "    # 5-6. 分析和可视化专家协作网络\n",
    "    expert_coop = analyze_expert_cooperation(layer_idx)\n",
    "    visualize_expert_cooperation(expert_coop, layer_idx)\n",
    "    \n",
    "    # 7-8. 分析和可视化注意力区域特化\n",
    "    df_region_attn = analyze_attention_specialization(expert_patterns, layer_idx)\n",
    "    visualize_attention_specialization(df_region_attn, layer_idx)\n",
    "\n",
    "print(\"所有分析完成！\")\n",
    "\n",
    "# 添加总结报告生成\n",
    "def generate_summary_report():\n",
    "    \"\"\"生成分析总结报告\"\"\"\n",
    "    print(\"生成总结报告...\")\n",
    "    \n",
    "    # 收集每层的主要统计数据\n",
    "    layer_stats = []\n",
    "    for layer_idx in range(num_layers):\n",
    "        # 分析专家使用频率\n",
    "        expert_counts = np.zeros(num_experts)\n",
    "        for seq_idx in range(num_seqs):\n",
    "            for pos in range(seq_len):\n",
    "                experts = topk_idx[layer_idx, seq_idx, pos]\n",
    "                for eid in experts:\n",
    "                    expert_counts[eid] += 1\n",
    "        \n",
    "        # 计算专家使用分布的熵\n",
    "        probs = expert_counts / (expert_counts.sum() + 1e-10)\n",
    "        entropy_val = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "        max_entropy = np.log2(num_experts)\n",
    "        \n",
    "        # 添加到统计数据\n",
    "        layer_stats.append({\n",
    "            'layer': layer_idx + 1,\n",
    "            'active_experts': np.sum(expert_counts > 0),\n",
    "            'entropy': entropy_val,\n",
    "            'entropy_ratio': entropy_val / max_entropy,\n",
    "            'max_expert': np.argmax(expert_counts),\n",
    "            'max_expert_ratio': np.max(expert_counts) / expert_counts.sum()\n",
    "        })\n",
    "    \n",
    "    # 创建报告表格\n",
    "    df_stats = pd.DataFrame(layer_stats)\n",
    "    \n",
    "    # 绘制统计图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 活跃专家数\n",
    "    axes[0, 0].plot(df_stats['layer'], df_stats['active_experts'], 'o-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Layer', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Active Experts', fontsize=12)\n",
    "    axes[0, 0].set_title('Number of Active Experts per Layer', fontsize=14)\n",
    "    axes[0, 0].grid(True, linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_xticks(df_stats['layer'])\n",
    "    \n",
    "    # 熵比率\n",
    "    axes[0, 1].plot(df_stats['layer'], df_stats['entropy_ratio'], 'o-', linewidth=2, color='orange')\n",
    "    axes[0, 1].set_xlabel('Layer', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Entropy Ratio', fontsize=12)\n",
    "    axes[0, 1].set_title('Expert Usage Entropy Ratio per Layer', fontsize=14)\n",
    "    axes[0, 1].grid(True, linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_xticks(df_stats['layer'])\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 最活跃的专家\n",
    "    axes[1, 0].bar(df_stats['layer'], df_stats['max_expert'], color='green')\n",
    "    axes[1, 0].set_xlabel('Layer', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Expert ID', fontsize=12)\n",
    "    axes[1, 0].set_title('Most Active Expert ID per Layer', fontsize=14)\n",
    "    axes[1, 0].grid(True, linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_xticks(df_stats['layer'])\n",
    "    \n",
    "    # 最活跃专家的比例\n",
    "    axes[1, 1].bar(df_stats['layer'], df_stats['max_expert_ratio'], color='purple')\n",
    "    axes[1, 1].set_xlabel('Layer', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Ratio', fontsize=12)\n",
    "    axes[1, 1].set_title('Most Active Expert Usage Ratio', fontsize=14)\n",
    "    axes[1, 1].grid(True, linestyle='--', alpha=0.7)\n",
    "    axes[1, 1].set_xticks(df_stats['layer'])\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.suptitle('MoE Information Flow Analysis Summary', fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    \n",
    "    # 保存总结图\n",
    "    summary_path = os.path.join(save_dir, \"moe_information_flow_summary.pdf\")\n",
    "    plt.savefig(summary_path, format='pdf', bbox_inches='tight')\n",
    "    print(f\"总结报告已保存至: {summary_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存数据表格\n",
    "    csv_path = os.path.join(save_dir, \"layer_statistics.csv\")\n",
    "    df_stats.to_csv(csv_path, index=False)\n",
    "    print(f\"统计数据已保存至: {csv_path}\")\n",
    "\n",
    "# 生成总结报告\n",
    "generate_summary_report()"
   ],
   "id": "9984857ae7a450b1",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T17:21:15.961444Z",
     "start_time": "2025-07-27T17:20:21.742938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/expert_evolution\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 加载数据\n",
    "topk_idx_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\"\n",
    "gate_scores_path = \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\"\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "topk_idx = np.load(topk_idx_path)  # (4, 1149, 300, 3)\n",
    "gate_scores = np.load(gate_scores_path)  # (4, 1149, 300, 30)\n",
    "\n",
    "num_layers, num_seqs, seq_len, _ = topk_idx.shape\n",
    "num_experts = gate_scores.shape[-1]\n",
    "topk = topk_idx.shape[-1]\n",
    "\n",
    "print(f\"数据加载完成: {num_layers}层, {num_experts}专家, 每个位置分配{topk}个专家\")\n",
    "\n",
    "def analyze_expert_evolution(topk_idx, gate_scores, save_dir):\n",
    "    \"\"\"分析专家角色如何随层数演化\"\"\"\n",
    "    num_layers = topk_idx.shape[0]\n",
    "    num_experts = gate_scores.shape[-1]\n",
    "    \n",
    "    print(f\"分析专家随层演化趋势...\")\n",
    "    \n",
    "    # 1. 计算每一层中专家的使用频率\n",
    "    layer_expert_usage = np.zeros((num_layers, num_experts))\n",
    "    layer_expert_importance = np.zeros((num_layers, num_experts))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        # 使用字典计数以提高效率\n",
    "        expert_counts = {eid: 0 for eid in range(num_experts)}\n",
    "        expert_scores = {eid: 0.0 for eid in range(num_experts)}\n",
    "        \n",
    "        for seq_idx in range(topk_idx.shape[1]):\n",
    "            for pos in range(topk_idx.shape[2]):\n",
    "                for k in range(topk_idx.shape[3]):\n",
    "                    eid = topk_idx[layer, seq_idx, pos, k]\n",
    "                    gate_value = gate_scores[layer, seq_idx, pos, eid]\n",
    "                    expert_counts[eid] += 1\n",
    "                    expert_scores[eid] += gate_value\n",
    "        \n",
    "        # 更新统计\n",
    "        for eid in range(num_experts):\n",
    "            layer_expert_usage[layer, eid] = expert_counts[eid]\n",
    "            layer_expert_importance[layer, eid] = expert_scores[eid]\n",
    "    \n",
    "    # 归一化每一层的使用频率\n",
    "    layer_expert_usage_norm = np.zeros_like(layer_expert_usage, dtype=float)\n",
    "    layer_expert_importance_norm = np.zeros_like(layer_expert_importance, dtype=float)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        usage_sum = layer_expert_usage[layer].sum()\n",
    "        importance_sum = layer_expert_importance[layer].sum()\n",
    "        \n",
    "        if usage_sum > 0:\n",
    "            layer_expert_usage_norm[layer] = layer_expert_usage[layer] / usage_sum\n",
    "        if importance_sum > 0:\n",
    "            layer_expert_importance_norm[layer] = layer_expert_importance[layer] / importance_sum\n",
    "    \n",
    "    # 2. 计算层与层之间的专家分工变化\n",
    "    layer_changes_usage = np.zeros(num_layers-1)\n",
    "    layer_changes_importance = np.zeros(num_layers-1)\n",
    "    \n",
    "    for i in range(num_layers-1):\n",
    "        # 使用L1距离度量变化\n",
    "        layer_changes_usage[i] = np.sum(np.abs(layer_expert_usage_norm[i+1] - layer_expert_usage_norm[i]))\n",
    "        layer_changes_importance[i] = np.sum(np.abs(layer_expert_importance_norm[i+1] - layer_expert_importance_norm[i]))\n",
    "    \n",
    "    # 3. 找出每层中最主要的专家\n",
    "    top_experts_per_layer = []\n",
    "    for layer in range(num_layers):\n",
    "        # 按重要性降序排列\n",
    "        top_indices = np.argsort(layer_expert_importance_norm[layer])[::-1]\n",
    "        cumulative = 0\n",
    "        selected = []\n",
    "        \n",
    "        # 选择累计重要性超过80%的专家\n",
    "        for idx in top_indices:\n",
    "            selected.append(idx)\n",
    "            cumulative += layer_expert_importance_norm[layer, idx]\n",
    "            if cumulative > 0.8:\n",
    "                break\n",
    "        \n",
    "        top_experts_per_layer.append(selected)\n",
    "    \n",
    "    # 4. 计算专家角色的稳定性\n",
    "    expert_stability = np.zeros(num_experts)\n",
    "    for eid in range(num_experts):\n",
    "        # 计算每层中该专家重要性的标准差\n",
    "        expert_stability[eid] = 1.0 / (np.std(layer_expert_importance_norm[:, eid]) + 1e-10)\n",
    "    \n",
    "    # 归一化稳定性得分\n",
    "    expert_stability = expert_stability / np.max(expert_stability)\n",
    "    \n",
    "    # 5. 计算层间专家转移矩阵\n",
    "    transition_matrices = []\n",
    "    for l in range(num_layers - 1):\n",
    "        trans_matrix = np.zeros((num_experts, num_experts))\n",
    "        \n",
    "        for seq_idx in range(topk_idx.shape[1]):\n",
    "            for pos in range(topk_idx.shape[2]):\n",
    "                # 当前层激活的专家\n",
    "                curr_experts = topk_idx[l, seq_idx, pos]\n",
    "                curr_scores = np.array([gate_scores[l, seq_idx, pos, e] for e in curr_experts])\n",
    "                \n",
    "                # 下一层激活的专家\n",
    "                next_experts = topk_idx[l+1, seq_idx, pos]\n",
    "                next_scores = np.array([gate_scores[l+1, seq_idx, pos, e] for e in next_experts])\n",
    "                \n",
    "                # 更新转移矩阵\n",
    "                for i, e1 in enumerate(curr_experts):\n",
    "                    for j, e2 in enumerate(next_experts):\n",
    "                        trans_matrix[e1, e2] += curr_scores[i] * next_scores[j]\n",
    "        \n",
    "        # 行归一化\n",
    "        row_sums = trans_matrix.sum(axis=1, keepdims=True)\n",
    "        norm_matrix = np.divide(trans_matrix, row_sums, where=row_sums!=0)\n",
    "        \n",
    "        transition_matrices.append(norm_matrix)\n",
    "    \n",
    "    # 保存结果为CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'expert_id': range(num_experts),\n",
    "        'stability': expert_stability\n",
    "    })\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        results_df[f'layer_{layer}_importance'] = layer_expert_importance_norm[layer]\n",
    "        results_df[f'layer_{layer}_usage'] = layer_expert_usage_norm[layer]\n",
    "    \n",
    "    results_df.to_csv(os.path.join(save_dir, 'expert_evolution_stats.csv'), index=False)\n",
    "    \n",
    "    # 返回计算结果\n",
    "    return {\n",
    "        'layer_expert_usage': layer_expert_usage,\n",
    "        'layer_expert_usage_norm': layer_expert_usage_norm,\n",
    "        'layer_expert_importance': layer_expert_importance,\n",
    "        'layer_expert_importance_norm': layer_expert_importance_norm,\n",
    "        'layer_changes_usage': layer_changes_usage,\n",
    "        'layer_changes_importance': layer_changes_importance,\n",
    "        'top_experts_per_layer': top_experts_per_layer,\n",
    "        'expert_stability': expert_stability,\n",
    "        'transition_matrices': transition_matrices\n",
    "    }\n",
    "\n",
    "def visualize_expert_evolution(results, save_dir):\n",
    "    \"\"\"可视化专家演化分析的结果\"\"\"\n",
    "    num_layers = results['layer_expert_usage'].shape[0]\n",
    "    num_experts = results['layer_expert_usage'].shape[1]\n",
    "    \n",
    "    # 1. 创建专家使用热图\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    # 自定义配色方案，从浅蓝到深蓝\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'custom_blues', \n",
    "        [(0, '#f7fbff'), (0.2, '#deebf7'), (0.4, '#c6dbef'), \n",
    "         (0.6, '#9ecae1'), (0.8, '#6baed6'), (1, '#08519c')], \n",
    "        N=256\n",
    "    )\n",
    "    \n",
    "    usage_data = results['layer_expert_usage_norm']\n",
    "    sns.heatmap(\n",
    "        usage_data, \n",
    "        cmap=cmap,\n",
    "        annot=False, \n",
    "        fmt='.2f',\n",
    "        linewidths=0.5, \n",
    "        cbar_kws={'label': 'Normalized Usage'},\n",
    "        xticklabels=range(num_experts),\n",
    "        yticklabels=[f'Layer {i+1}' for i in range(num_layers)]\n",
    "    )\n",
    "    \n",
    "    plt.title('Expert Usage Distribution Across Layers', fontsize=16)\n",
    "    plt.xlabel('Expert ID', fontsize=14)\n",
    "    plt.ylabel('', fontsize=14)\n",
    "    \n",
    "    # 2. 创建专家重要性热图\n",
    "    plt.subplot(2, 1, 2)\n",
    "    importance_data = results['layer_expert_importance_norm']\n",
    "    \n",
    "    # 高亮每一层中最重要的专家\n",
    "    mask = np.zeros_like(importance_data, dtype=bool)\n",
    "    for layer, top_experts in enumerate(results['top_experts_per_layer']):\n",
    "        mask[layer, top_experts] = True\n",
    "    \n",
    "    # 主热图\n",
    "    sns.heatmap(\n",
    "        importance_data, \n",
    "        cmap=cmap,\n",
    "        annot=False, \n",
    "        fmt='.2f',\n",
    "        linewidths=0.5, \n",
    "        cbar_kws={'label': 'Normalized Importance'},\n",
    "        xticklabels=range(num_experts),\n",
    "        yticklabels=[f'Layer {i+1}' for i in range(num_layers)]\n",
    "    )\n",
    "    \n",
    "    # 添加高亮边框\n",
    "    for layer in range(num_layers):\n",
    "        for expert in results['top_experts_per_layer'][layer]:\n",
    "            plt.gca().add_patch(plt.Rectangle((expert, layer), 1, 1, \n",
    "                           fill=False, edgecolor='red', lw=2))\n",
    "    \n",
    "    plt.title('Expert Importance Distribution Across Layers', fontsize=16)\n",
    "    plt.xlabel('Expert ID', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'expert_distribution_across_layers.pdf'), format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. 层间变化趋势图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(1, num_layers)\n",
    "    \n",
    "    plt.plot(x, results['layer_changes_usage'], 'o-', linewidth=2, markersize=10, \n",
    "             color='#1f77b4', label='Usage Change')\n",
    "    plt.plot(x, results['layer_changes_importance'], 's-', linewidth=2, markersize=10, \n",
    "             color='#ff7f0e', label='Importance Change')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, (u, imp) in enumerate(zip(results['layer_changes_usage'], results['layer_changes_importance'])):\n",
    "        plt.text(x[i], u+0.02, f\"{u:.2f}\", ha='center', fontsize=12)\n",
    "        plt.text(x[i], imp-0.04, f\"{imp:.2f}\", ha='center', fontsize=12)\n",
    "    \n",
    "    plt.xlabel('Layer Transition', fontsize=14)\n",
    "    plt.ylabel('Distribution Change (L1 Distance)', fontsize=14)\n",
    "    plt.title('Expert Role Shift Between Consecutive Layers', fontsize=16)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(x, [f'Layer {i} → {i+1}' for i in x])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'expert_role_shift_between_layers.pdf'), format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. 专家稳定性条形图\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    stability = results['expert_stability']\n",
    "    \n",
    "    # 根据稳定性得分对专家进行排序\n",
    "    sorted_indices = np.argsort(stability)[::-1]\n",
    "    sorted_stability = stability[sorted_indices]\n",
    "    \n",
    "    # 创建渐变颜色\n",
    "    colors = plt.cm.viridis(sorted_stability)\n",
    "    \n",
    "    plt.bar(range(num_experts), sorted_stability, color=colors)\n",
    "    plt.title('Expert Stability Across Layers', fontsize=16)\n",
    "    plt.xlabel('Expert ID (Sorted by Stability)', fontsize=14)\n",
    "    plt.ylabel('Normalized Stability Score', fontsize=14)\n",
    "    plt.xticks(range(num_experts), [f'E{i}' for i in sorted_indices], rotation=90)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 添加标签\n",
    "    for i, (idx, stab) in enumerate(zip(sorted_indices, sorted_stability)):\n",
    "        if stab > 0.5:  # 只标注稳定性较高的专家\n",
    "            plt.text(i, stab + 0.03, f\"E{idx}\", ha='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'expert_stability_across_layers.pdf'), format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. 层间专家转移可视化 (选择前两个层间转移)\n",
    "    for l in range(min(2, len(results['transition_matrices']))):\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        trans_matrix = results['transition_matrices'][l]\n",
    "        \n",
    "        # 只显示非零转移\n",
    "        mask = trans_matrix < 0.05\n",
    "        \n",
    "        sns.heatmap(\n",
    "            trans_matrix, \n",
    "            cmap=\"YlGnBu\",\n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            linewidths=0.5,\n",
    "            mask=mask,\n",
    "            cbar_kws={'label': 'Transition Probability'},\n",
    "            xticklabels=[f'E{i}' for i in range(num_experts)],\n",
    "            yticklabels=[f'E{i}' for i in range(num_experts)]\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Expert Transition Matrix: Layer {l+1} → Layer {l+2}', fontsize=16)\n",
    "        plt.xlabel(f'Experts in Layer {l+2}', fontsize=14)\n",
    "        plt.ylabel(f'Experts in Layer {l+1}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'expert_transition_layer{l+1}_to_layer{l+2}.pdf'), format='pdf')\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. 创建专家演化桑基图\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        \n",
    "        # 选择最重要的专家和最重要的转移\n",
    "        top_k_experts = 10\n",
    "        \n",
    "        # 获取每层中最重要的专家\n",
    "        important_experts = []\n",
    "        for layer in range(num_layers):\n",
    "            importance = results['layer_expert_importance_norm'][layer]\n",
    "            top_indices = np.argsort(importance)[-top_k_experts:]\n",
    "            important_experts.extend([f\"L{layer+1}_E{e}\" for e in top_indices])\n",
    "        \n",
    "        # 去除重复\n",
    "        important_experts = list(set(important_experts))\n",
    "        \n",
    "        # 创建节点\n",
    "        nodes = important_experts\n",
    "        node_color = []\n",
    "        for node in nodes:\n",
    "            layer = int(node.split('_')[0][1:]) - 1\n",
    "            expert = int(node.split('_')[1][1:])\n",
    "            importance = results['layer_expert_importance_norm'][layer, expert]\n",
    "            node_color.append(f\"rgba(31, 119, 180, {0.3 + 0.7*importance})\")\n",
    "        \n",
    "        # 创建连接\n",
    "        links_source = []\n",
    "        links_target = []\n",
    "        links_value = []\n",
    "        links_color = []\n",
    "        \n",
    "        for l in range(num_layers - 1):\n",
    "            trans_matrix = results['transition_matrices'][l]\n",
    "            \n",
    "            # 获取重要连接\n",
    "            for i in range(num_experts):\n",
    "                for j in range(num_experts):\n",
    "                    src_node = f\"L{l+1}_E{i}\"\n",
    "                    tgt_node = f\"L{l+2}_E{j}\"\n",
    "                    \n",
    "                    if src_node in nodes and tgt_node in nodes:\n",
    "                        value = trans_matrix[i, j]\n",
    "                        if value > 0.1:  # 只显示较强的连接\n",
    "                            src_idx = nodes.index(src_node)\n",
    "                            tgt_idx = nodes.index(tgt_node)\n",
    "                            links_source.append(src_idx)\n",
    "                            links_target.append(tgt_idx)\n",
    "                            links_value.append(value * 10)  # 放大连接值使其更明显\n",
    "                            links_color.append(f\"rgba(255, 127, 14, {value})\")\n",
    "        \n",
    "        # 创建桑基图\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = \"black\", width = 0.5),\n",
    "                label = nodes,\n",
    "                color = node_color\n",
    "            ),\n",
    "            link = dict(\n",
    "                source = links_source,\n",
    "                target = links_target,\n",
    "                value = links_value,\n",
    "                color = links_color\n",
    "            )\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title_text=\"Expert Evolution Across Layers (Top Important Experts)\",\n",
    "            font_size=12,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        fig.write_html(os.path.join(save_dir, 'expert_evolution_sankey.html'))\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"plotly 库未安装，跳过桑基图生成\")\n",
    "\n",
    "# 执行分析\n",
    "print(\"执行专家演化分析...\")\n",
    "results = analyze_expert_evolution(topk_idx, gate_scores, save_dir)\n",
    "\n",
    "# 可视化结果\n",
    "print(\"可视化分析结果...\")\n",
    "visualize_expert_evolution(results, save_dir)\n",
    "\n",
    "print(f\"分析完成，结果保存在 {save_dir}\")\n",
    "\n",
    "# 生成摘要报告\n",
    "def generate_summary_report(results, save_dir):\n",
    "    \"\"\"生成专家演化分析的摘要报告\"\"\"\n",
    "    num_experts = results['layer_expert_usage'].shape[1]\n",
    "    num_layers = results['layer_expert_usage'].shape[0]\n",
    "    \n",
    "    # 提取关键洞察\n",
    "    most_important_experts = []\n",
    "    for layer in range(num_layers):\n",
    "        importance = results['layer_expert_importance_norm'][layer]\n",
    "        # 获取前3个最重要的专家\n",
    "        top3 = np.argsort(importance)[-3:][::-1]\n",
    "        importance_values = importance[top3]\n",
    "        most_important_experts.append((layer, top3, importance_values))\n",
    "    \n",
    "    # 找出最稳定的专家\n",
    "    stability = results['expert_stability']\n",
    "    most_stable = np.argsort(stability)[-5:][::-1]\n",
    "    \n",
    "    # 找出角色变化最大的层间转换\n",
    "    changes = results['layer_changes_importance']\n",
    "    max_change_idx = np.argmax(changes)\n",
    "    \n",
    "    # 生成摘要HTML报告\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>专家演化分析报告</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            h1 {{ color: #2c3e50; }}\n",
    "            h2 {{ color: #3498db; }}\n",
    "            .insight {{ background-color: #f8f9fa; padding: 15px; margin: 10px 0; border-left: 5px solid #3498db; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; }}\n",
    "            th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            tr:hover {{ background-color: #f5f5f5; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>MOE专家演化分析报告</h1>\n",
    "        \n",
    "        <h2>主要发现</h2>\n",
    "        \n",
    "        <div class=\"insight\">\n",
    "            <h3>关键专家</h3>\n",
    "            <p>各层中最重要的专家:</p>\n",
    "            <table>\n",
    "                <tr><th>层</th><th>最重要专家</th><th>重要性值</th></tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for layer, top3, values in most_important_experts:\n",
    "        experts_str = \", \".join([f\"Expert {e} ({values[i]:.3f})\" for i, e in enumerate(top3)])\n",
    "        html += f\"<tr><td>Layer {layer+1}</td><td>{experts_str}</td><td>{values.sum():.3f}</td></tr>\"\n",
    "    \n",
    "    html += f\"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"insight\">\n",
    "            <h3>专家稳定性</h3>\n",
    "            <p>最稳定的5个专家 (在不同层中保持一致的角色):</p>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    for e in most_stable:\n",
    "        html += f\"<li>Expert {e} (稳定性: {stability[e]:.3f})</li>\"\n",
    "    \n",
    "    html += f\"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"insight\">\n",
    "            <h3>层间变化</h3>\n",
    "            <p>最大的专家角色转变发生在 Layer {max_change_idx+1} → Layer {max_change_idx+2} (变化值: {changes[max_change_idx]:.3f})</p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>可视化</h2>\n",
    "        <p>详细的可视化结果请查看保存的PDF文件。</p>\n",
    "        \n",
    "        <h2>结论</h2>\n",
    "        <p>MOE模型中的专家在不同层展现出明显的专业化和演化模式。早期层专注于基础特征提取，而较深的层则更多关注高级语义模式。\n",
    "        某些专家在整个网络中保持稳定的角色，而另一些则在不同层之间转变其功能。</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 保存HTML报告\n",
    "    with open(os.path.join(save_dir, 'expert_evolution_report.html'), 'w') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    print(f\"摘要报告已保存至 {os.path.join(save_dir, 'expert_evolution_report.html')}\")\n",
    "\n",
    "# 生成摘要报告\n",
    "generate_summary_report(results, save_dir)"
   ],
   "id": "5bd71542af8873d7",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T17:43:14.698615Z",
     "start_time": "2025-07-27T17:42:47.192196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = \"moe_analysis/expert_evolution\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def create_expert_evolution_sankey(topk_idx, gate_scores, save_path):\n",
    "    \"\"\"\n",
    "    创建展示专家演化的桑基图\n",
    "    修正了层级顺序问题，确保每列只包含对应层的专家\n",
    "    \n",
    "    参数:\n",
    "    topk_idx: 每个位置的top-k专家索引\n",
    "    gate_scores: 门控分数\n",
    "    save_path: 保存路径\n",
    "    \"\"\"\n",
    "    num_layers, num_seqs, seq_len, _ = topk_idx.shape\n",
    "    num_experts = gate_scores.shape[-1]\n",
    "    \n",
    "    # 计算每一层中专家的重要性\n",
    "    layer_expert_importance = np.zeros((num_layers, num_experts))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for seq_idx in range(topk_idx.shape[1]):\n",
    "            for pos in range(topk_idx.shape[2]):\n",
    "                for k in range(topk_idx.shape[3]):\n",
    "                    eid = topk_idx[layer, seq_idx, pos, k]\n",
    "                    gate_value = gate_scores[layer, seq_idx, pos, eid]\n",
    "                    layer_expert_importance[layer, eid] += gate_value\n",
    "    \n",
    "    # 归一化重要性\n",
    "    for layer in range(num_layers):\n",
    "        total = layer_expert_importance[layer].sum()\n",
    "        if total > 0:\n",
    "            layer_expert_importance[layer] /= total\n",
    "    \n",
    "    # 选择每一层中的重要专家 (确保每层至少选择6个专家)\n",
    "    top_k_per_layer = min(6, num_experts)\n",
    "    selected_experts = []\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        # 按重要性降序排列\n",
    "        indices = np.argsort(layer_expert_importance[layer])[::-1]\n",
    "        # 选择前top_k_per_layer个\n",
    "        layer_experts = [f\"L{layer+1}_E{idx}\" for idx in indices[:top_k_per_layer]]\n",
    "        selected_experts.append(layer_experts)\n",
    "    \n",
    "    # 扁平化为一维列表\n",
    "    all_experts = []\n",
    "    for layer_experts in selected_experts:\n",
    "        all_experts.extend(layer_experts)\n",
    "    \n",
    "    # 计算层间转移关系\n",
    "    links = []\n",
    "    \n",
    "    for l in range(num_layers - 1):\n",
    "        src_experts = selected_experts[l]\n",
    "        tgt_experts = selected_experts[l+1]\n",
    "        \n",
    "        # 创建从当前层到下一层的转移矩阵\n",
    "        trans_matrix = np.zeros((len(src_experts), len(tgt_experts)))\n",
    "        \n",
    "        for seq_idx in range(num_seqs):\n",
    "            for pos in range(seq_len):\n",
    "                # 当前层激活的专家及其分数\n",
    "                curr_experts = [(topk_idx[l, seq_idx, pos, k], \n",
    "                                gate_scores[l, seq_idx, pos, topk_idx[l, seq_idx, pos, k]]) \n",
    "                               for k in range(topk_idx.shape[3])]\n",
    "                \n",
    "                # 下一层激活的专家及其分数\n",
    "                next_experts = [(topk_idx[l+1, seq_idx, pos, k], \n",
    "                               gate_scores[l+1, seq_idx, pos, topk_idx[l+1, seq_idx, pos, k]]) \n",
    "                              for k in range(topk_idx.shape[3])]\n",
    "                \n",
    "                # 对每对专家，累加转移权重\n",
    "                for curr_e, curr_score in curr_experts:\n",
    "                    curr_name = f\"L{l+1}_E{curr_e}\"\n",
    "                    if curr_name not in src_experts:\n",
    "                        continue\n",
    "                        \n",
    "                    for next_e, next_score in next_experts:\n",
    "                        next_name = f\"L{l+2}_E{next_e}\"\n",
    "                        if next_name not in tgt_experts:\n",
    "                            continue\n",
    "                            \n",
    "                        # 计算连接权重\n",
    "                        weight = curr_score * next_score\n",
    "                        \n",
    "                        # 更新转移矩阵\n",
    "                        i = src_experts.index(curr_name)\n",
    "                        j = tgt_experts.index(next_name)\n",
    "                        trans_matrix[i, j] += weight\n",
    "        \n",
    "        # 将转移矩阵转换为链接列表\n",
    "        for i, src in enumerate(src_experts):\n",
    "            for j, tgt in enumerate(tgt_experts):\n",
    "                # 仅添加有意义的连接\n",
    "                if trans_matrix[i, j] > 0.01:  # 阈值可调\n",
    "                    src_idx = all_experts.index(src)\n",
    "                    tgt_idx = all_experts.index(tgt)\n",
    "                    # 放大值使其在图中更明显\n",
    "                    value = float(trans_matrix[i, j] * 100)\n",
    "                    links.append((src_idx, tgt_idx, value))\n",
    "    \n",
    "    # 准备Sankey图的数据\n",
    "    node_labels = all_experts\n",
    "    link_sources = [link[0] for link in links]\n",
    "    link_targets = [link[1] for link in links]\n",
    "    link_values = [link[2] for link in links]\n",
    "    \n",
    "    # 为不同层的节点设置不同的颜色\n",
    "    node_colors = []\n",
    "    for node in all_experts:\n",
    "        layer = int(node.split('_')[0][1:])\n",
    "        # 使用渐变色方案\n",
    "        if layer == 1:\n",
    "            node_colors.append(\"rgba(31, 119, 180, 0.9)\")  # 蓝色\n",
    "        elif layer == 2:\n",
    "            node_colors.append(\"rgba(44, 160, 44, 0.9)\")   # 绿色\n",
    "        elif layer == 3:\n",
    "            node_colors.append(\"rgba(214, 39, 40, 0.9)\")   # 红色\n",
    "        else:\n",
    "            node_colors.append(\"rgba(148, 103, 189, 0.9)\") # 紫色\n",
    "    \n",
    "    # 为连接设置渐变颜色\n",
    "    link_colors = []\n",
    "    for src, tgt, val in links:\n",
    "        # 获取源节点和目标节点的层\n",
    "        src_layer = int(all_experts[src].split('_')[0][1:])\n",
    "        tgt_layer = int(all_experts[tgt].split('_')[0][1:])\n",
    "        \n",
    "        # 基于值的大小设置透明度和颜色强度\n",
    "        strength = min(1.0, val / 50)  # 归一化到0-1范围\n",
    "        \n",
    "        # 使用从金色到橙色的渐变\n",
    "        if strength > 0.7:\n",
    "            link_colors.append(f\"rgba(255, 165, 0, {0.7 + 0.3*strength})\")  # 橙色，强连接\n",
    "        elif strength > 0.4:\n",
    "            link_colors.append(f\"rgba(255, 140, 0, {0.5 + 0.3*strength})\")  # 深橙色，中等连接\n",
    "        else:\n",
    "            link_colors.append(f\"rgba(210, 105, 30, {0.3 + 0.3*strength})\")  # 棕色，弱连接\n",
    "    \n",
    "    # 创建Sankey图\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 20,\n",
    "            line = dict(color = \"black\", width = 0.5),\n",
    "            label = node_labels,\n",
    "            color = node_colors\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = link_sources,\n",
    "            target = link_targets,\n",
    "            value = link_values,\n",
    "            color = link_colors\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    # 调整布局\n",
    "    fig.update_layout(\n",
    "        title_text=\"Expert Evolution Across Layers\",\n",
    "        font_size=12,\n",
    "        height=800,\n",
    "        paper_bgcolor='rgba(240, 240, 240, 0.9)',  # 浅灰背景\n",
    "        plot_bgcolor='rgba(240, 240, 240, 0.9)'\n",
    "    )\n",
    "    \n",
    "    # 保存为HTML\n",
    "    fig.write_html(save_path)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 假设这些是从之前加载的数据\n",
    "# topk_idx = np.load(\"topk_idx.npy\")\n",
    "# gate_scores = np.load(\"gate_scores.npy\")\n",
    "\n",
    "# 使用示例\n",
    "# create_expert_evolution_sankey(\n",
    "#     topk_idx, \n",
    "#     gate_scores, \n",
    "#     os.path.join(save_dir, 'improved_expert_evolution_sankey.html')\n",
    "# )\n",
    "\n",
    "def run_sankey_analysis(topk_idx_path, gate_scores_path, save_dir):\n",
    "    \"\"\"完整的桑基图分析流程\"\"\"\n",
    "    print(\"加载数据中...\")\n",
    "    topk_idx = np.load(topk_idx_path)  \n",
    "    gate_scores = np.load(gate_scores_path)\n",
    "    \n",
    "    print(\"创建专家演化桑基图...\")\n",
    "    create_expert_evolution_sankey(\n",
    "        topk_idx,\n",
    "        gate_scores,\n",
    "        os.path.join(save_dir, 'expert_evolution_sankey.html')\n",
    "    )\n",
    "    \n",
    "    print(f\"分析完成，结果保存在 {save_dir}\")\n",
    "\n",
    "# 调用示例\n",
    "run_sankey_analysis(\n",
    "     \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\",\n",
    "     \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\",\n",
    "     \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/expert_evolution\"\n",
    ")"
   ],
   "id": "c4eb3129cee694fc",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:59:26.212002Z",
     "start_time": "2025-07-28T04:59:08.170798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = \"moe_analysis/expert_evolution\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def create_expert_evolution_sankey(topk_idx, gate_scores, token_types, save_path):\n",
    "    \"\"\"\n",
    "    创建展示专家演化的桑基图，正确处理padding\n",
    "    \n",
    "    参数:\n",
    "    topk_idx: 每个位置的top-k专家索引\n",
    "    gate_scores: 门控分数\n",
    "    token_types: 序列中的token类型，padding处为特殊值\n",
    "    save_path: 保存路径\n",
    "    \"\"\"\n",
    "    num_layers, num_seqs, seq_len, _ = topk_idx.shape\n",
    "    num_experts = gate_scores.shape[-1]\n",
    "    \n",
    "    # 首先确定padding的标识符 - 通常是-1，但也可能是其他特殊值\n",
    "    # 我们通过检查每个序列的填充模式来确定\n",
    "    padding_value = -1  # 默认假设是-1\n",
    "    \n",
    "    # 查看一些样本，判断padding值\n",
    "    for seq_idx in range(min(10, num_seqs)):\n",
    "        # 查看序列末尾的值\n",
    "        end_values = token_types[seq_idx, -10:]\n",
    "        # 如果末尾有连续相同的值，可能是padding\n",
    "        if len(set(end_values)) == 1 and end_values[0] != -1:\n",
    "            padding_value = end_values[0]\n",
    "            print(f\"检测到可能的padding值: {padding_value}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"使用 {padding_value} 作为padding标识符\")\n",
    "    \n",
    "    # 计算每一层中专家的重要性，跳过padding位置\n",
    "    layer_expert_importance = np.zeros((num_layers, num_experts))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for seq_idx in range(topk_idx.shape[1]):\n",
    "            # 确定该序列的有效长度（非padding部分）\n",
    "            valid_mask = token_types[seq_idx] != padding_value\n",
    "            valid_len = np.sum(valid_mask)\n",
    "            \n",
    "            if valid_len == 0:\n",
    "                continue  # 跳过完全是padding的序列\n",
    "                \n",
    "            for pos in range(seq_len):\n",
    "                # 只处理非padding位置\n",
    "                if not valid_mask[pos]:\n",
    "                    continue\n",
    "                    \n",
    "                for k in range(topk_idx.shape[3]):\n",
    "                    eid = topk_idx[layer, seq_idx, pos, k]\n",
    "                    gate_value = gate_scores[layer, seq_idx, pos, eid]\n",
    "                    layer_expert_importance[layer, eid] += gate_value\n",
    "    \n",
    "    # 归一化重要性\n",
    "    for layer in range(num_layers):\n",
    "        total = layer_expert_importance[layer].sum()\n",
    "        if total > 0:\n",
    "            layer_expert_importance[layer] /= total\n",
    "    \n",
    "    # 选择每一层中的重要专家 (确保每层至少选择6个专家)\n",
    "    top_k_per_layer = min(6, num_experts)\n",
    "    selected_experts = []\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        # 按重要性降序排列\n",
    "        indices = np.argsort(layer_expert_importance[layer])[::-1]\n",
    "        # 选择前top_k_per_layer个\n",
    "        layer_experts = [f\"L{layer+1}_E{idx}\" for idx in indices[:top_k_per_layer]]\n",
    "        selected_experts.append(layer_experts)\n",
    "    \n",
    "    # 扁平化为一维列表\n",
    "    all_experts = []\n",
    "    for layer_experts in selected_experts:\n",
    "        all_experts.extend(layer_experts)\n",
    "    \n",
    "    # 计算层间转移关系，跳过padding位置\n",
    "    links = []\n",
    "    \n",
    "    for l in range(num_layers - 1):\n",
    "        src_experts = selected_experts[l]\n",
    "        tgt_experts = selected_experts[l+1]\n",
    "        \n",
    "        # 创建从当前层到下一层的转移矩阵\n",
    "        trans_matrix = np.zeros((len(src_experts), len(tgt_experts)))\n",
    "        \n",
    "        for seq_idx in range(num_seqs):\n",
    "            # 确定该序列的有效长度（非padding部分）\n",
    "            valid_mask = token_types[seq_idx] != padding_value\n",
    "            \n",
    "            for pos in range(seq_len):\n",
    "                # 只处理非padding位置\n",
    "                if not valid_mask[pos]:\n",
    "                    continue\n",
    "                \n",
    "                # 当前层激活的专家及其分数\n",
    "                curr_experts = [(topk_idx[l, seq_idx, pos, k], \n",
    "                                gate_scores[l, seq_idx, pos, topk_idx[l, seq_idx, pos, k]]) \n",
    "                               for k in range(topk_idx.shape[3])]\n",
    "                \n",
    "                # 下一层激活的专家及其分数\n",
    "                next_experts = [(topk_idx[l+1, seq_idx, pos, k], \n",
    "                               gate_scores[l+1, seq_idx, pos, topk_idx[l+1, seq_idx, pos, k]]) \n",
    "                              for k in range(topk_idx.shape[3])]\n",
    "                \n",
    "                # 对每对专家，累加转移权重\n",
    "                for curr_e, curr_score in curr_experts:\n",
    "                    curr_name = f\"L{l+1}_E{curr_e}\"\n",
    "                    if curr_name not in src_experts:\n",
    "                        continue\n",
    "                        \n",
    "                    for next_e, next_score in next_experts:\n",
    "                        next_name = f\"L{l+2}_E{next_e}\"\n",
    "                        if next_name not in tgt_experts:\n",
    "                            continue\n",
    "                            \n",
    "                        # 计算连接权重\n",
    "                        weight = curr_score * next_score\n",
    "                        \n",
    "                        # 更新转移矩阵\n",
    "                        i = src_experts.index(curr_name)\n",
    "                        j = tgt_experts.index(next_name)\n",
    "                        trans_matrix[i, j] += weight\n",
    "        \n",
    "        # 将转移矩阵转换为链接列表\n",
    "        for i, src in enumerate(src_experts):\n",
    "            for j, tgt in enumerate(tgt_experts):\n",
    "                # 仅添加有意义的连接\n",
    "                if trans_matrix[i, j] > 0.01:  # 阈值可调\n",
    "                    src_idx = all_experts.index(src)\n",
    "                    tgt_idx = all_experts.index(tgt)\n",
    "                    # 放大值使其在图中更明显\n",
    "                    value = float(trans_matrix[i, j] * 100)\n",
    "                    links.append((src_idx, tgt_idx, value))\n",
    "    \n",
    "    # 准备Sankey图的数据\n",
    "    node_labels = all_experts\n",
    "    link_sources = [link[0] for link in links]\n",
    "    link_targets = [link[1] for link in links]\n",
    "    link_values = [link[2] for link in links]\n",
    "    \n",
    "    # 为不同层的节点设置不同的颜色\n",
    "    node_colors = []\n",
    "    for node in all_experts:\n",
    "        layer = int(node.split('_')[0][1:])\n",
    "        # 使用渐变色方案\n",
    "        if layer == 1:\n",
    "            node_colors.append(\"rgba(31, 119, 180, 0.9)\")  # 蓝色\n",
    "        elif layer == 2:\n",
    "            node_colors.append(\"rgba(44, 160, 44, 0.9)\")   # 绿色\n",
    "        elif layer == 3:\n",
    "            node_colors.append(\"rgba(214, 39, 40, 0.9)\")   # 红色\n",
    "        else:\n",
    "            node_colors.append(\"rgba(148, 103, 189, 0.9)\") # 紫色\n",
    "    \n",
    "    # 为连接设置渐变颜色\n",
    "    link_colors = []\n",
    "    for src, tgt, val in links:\n",
    "        # 获取源节点和目标节点的层\n",
    "        src_layer = int(all_experts[src].split('_')[0][1:])\n",
    "        tgt_layer = int(all_experts[tgt].split('_')[0][1:])\n",
    "        \n",
    "        # 基于值的大小设置透明度和颜色强度\n",
    "        strength = min(1.0, val / 50)  # 归一化到0-1范围\n",
    "        \n",
    "        # 使用从金色到橙色的渐变\n",
    "        if strength > 0.7:\n",
    "            link_colors.append(f\"rgba(255, 165, 0, {0.7 + 0.3*strength})\")  # 橙色，强连接\n",
    "        elif strength > 0.4:\n",
    "            link_colors.append(f\"rgba(255, 140, 0, {0.5 + 0.3*strength})\")  # 深橙色，中等连接\n",
    "        else:\n",
    "            link_colors.append(f\"rgba(210, 105, 30, {0.3 + 0.3*strength})\")  # 棕色，弱连接\n",
    "    \n",
    "    # 创建Sankey图\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 20,\n",
    "            line = dict(color = \"black\", width = 0.5),\n",
    "            label = node_labels,\n",
    "            color = node_colors\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = link_sources,\n",
    "            target = link_targets,\n",
    "            value = link_values,\n",
    "            color = link_colors\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    # 调整布局\n",
    "    fig.update_layout(\n",
    "        title_text=\"Expert Evolution Across Layers\",\n",
    "        font_size=12,\n",
    "        height=800,\n",
    "        paper_bgcolor='rgba(240, 240, 240, 0.9)',  # 浅灰背景\n",
    "        plot_bgcolor='rgba(240, 240, 240, 0.9)'\n",
    "    )\n",
    "    \n",
    "    # 保存为HTML\n",
    "    fig.write_html(save_path)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def run_sankey_analysis(topk_idx_path, gate_scores_path, token_types_path, save_dir):\n",
    "    \"\"\"完整的桑基图分析流程\"\"\"\n",
    "    print(\"加载数据中...\")\n",
    "    topk_idx = np.load(topk_idx_path)  \n",
    "    gate_scores = np.load(gate_scores_path)\n",
    "    token_types = np.load(token_types_path)\n",
    "    \n",
    "    print(f\"数据形状: topk_idx={topk_idx.shape}, gate_scores={gate_scores.shape}, token_types={token_types.shape}\")\n",
    "    \n",
    "    print(\"创建专家演化桑基图...\")\n",
    "    create_expert_evolution_sankey(\n",
    "        topk_idx,\n",
    "        gate_scores,\n",
    "        token_types,\n",
    "        os.path.join(save_dir, 'expert_evolution_sankey.html')\n",
    "    )\n",
    "    \n",
    "    print(f\"分析完成，结果保存在 {save_dir}\")\n",
    "\n",
    "# 主函数调用\n",
    "if __name__ == \"__main__\":\n",
    "    run_sankey_analysis(\n",
    "        \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\",\n",
    "        \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\",\n",
    "        \"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/test_token_types.npy\",\n",
    "        \"/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/expert_evolution\"\n",
    "    )"
   ],
   "id": "f07bb1f3cab06569",
   "execution_count": 2,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
