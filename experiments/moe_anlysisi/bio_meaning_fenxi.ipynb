{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-17T03:25:21.693220Z",
     "start_time": "2025-07-17T03:25:19.651411Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_fasta(filepath):\n",
    "    \"\"\"读取fasta文件\"\"\"\n",
    "    seqs = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        seq = ''\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if seq:\n",
    "                    seqs.append(seq)\n",
    "                    seq = ''\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq:\n",
    "            seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "\n",
    "# 读取测试集序列\n",
    "pos_test_seqs = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/positive_test.fasta')\n",
    "neg_test_seqs = read_fasta('/exp_data/sjx/star/first_data/shisuandanbai/negative_test.fasta')\n",
    "\n",
    "print(f\"正样本测试集序列数: {len(pos_test_seqs)}\")\n",
    "print(f\"负样本测试集序列数: {len(neg_test_seqs)}\")\n",
    "\n",
    "# 加载模型权重数据\n",
    "attn_weights = np.load(\"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/attn_weights.npy\")\n",
    "gate_scores = np.load(\"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/gate_scores.npy\")\n",
    "topk_idx = np.load(\"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/topk_idx.npy\")\n",
    "labels = np.load(\"/exp_data/sjx/star/main_transformer_moe_weight/experiment_data/labels.npy\")\n",
    "\n",
    "print(f\"数据形状:\")\n",
    "print(f\"  attn_weights: {attn_weights.shape}\")\n",
    "print(f\"  gate_scores: {gate_scores.shape}\")\n",
    "print(f\"  topk_idx: {topk_idx.shape}\")\n",
    "print(f\"  labels: {labels.shape}\")\n",
    "\n",
    "# 选择第一条正样本序列进行分析\n",
    "seq_idx = 1  # 第一条正样本\n",
    "layer_idx = 0  # 第一层\n",
    "target_seq = pos_test_seqs[seq_idx]\n",
    "seq_len = len(target_seq)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"序列详细信息\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"序列索引: {seq_idx}\")\n",
    "print(f\"序列长度: {seq_len}\")\n",
    "print(f\"序列标签: {labels[seq_idx]} (1=正样本)\")\n",
    "print(f\"完整序列: {target_seq}\")\n",
    "print(f\"序列前50个氨基酸: {target_seq[:50]}\")\n",
    "print(f\"序列后50个氨基酸: {target_seq[-50:] if seq_len > 50 else target_seq}\")\n",
    "\n",
    "# 氨基酸组成统计\n",
    "aa_counts = Counter(target_seq)\n",
    "print(f\"\\n氨基酸组成:\")\n",
    "for aa in sorted(aa_counts.keys()):\n",
    "    print(f\"  {aa}: {aa_counts[aa]} ({aa_counts[aa] / seq_len * 100:.1f}%)\")\n",
    "\n",
    "# 检查注意力权重的实际形状\n",
    "print(f\"\\n注意力权重形状检查:\")\n",
    "print(f\"  attn_weights[layer_idx, seq_idx].shape: {attn_weights[layer_idx, seq_idx].shape}\")\n",
    "print(f\"  attn_weights[layer_idx, seq_idx].ndim: {attn_weights[layer_idx, seq_idx].ndim}\")\n",
    "\n",
    "# 根据实际形状获取注意力权重\n",
    "if attn_weights[layer_idx, seq_idx].ndim == 3:\n",
    "    # 如果是 [heads, seq_len, seq_len]\n",
    "    seq_attn = attn_weights[layer_idx, seq_idx].mean(axis=0)  # [seq_len, seq_len]\n",
    "elif attn_weights[layer_idx, seq_idx].ndim == 2:\n",
    "    # 如果已经是 [seq_len, seq_len]\n",
    "    seq_attn = attn_weights[layer_idx, seq_idx]\n",
    "else:\n",
    "    print(f\"意外的注意力权重形状: {attn_weights[layer_idx, seq_idx].shape}\")\n",
    "    # 尝试重塑\n",
    "    seq_attn = attn_weights[layer_idx, seq_idx].reshape(300, 300)  # 假设最大长度是300\n",
    "\n",
    "seq_gate_scores = gate_scores[layer_idx, seq_idx]  # [seq_len, num_experts]\n",
    "seq_expert_assign = topk_idx[layer_idx, seq_idx]  # [seq_len, topk]\n",
    "\n",
    "print(f\"\\n处理后的数据形状:\")\n",
    "print(f\"  seq_attn: {seq_attn.shape}\")\n",
    "print(f\"  seq_gate_scores: {seq_gate_scores.shape}\")\n",
    "print(f\"  seq_expert_assign: {seq_expert_assign.shape}\")\n",
    "\n",
    "# 只取实际序列长度的部分（去除padding）\n",
    "seq_attn = seq_attn[:seq_len, :seq_len]\n",
    "seq_gate_scores = seq_gate_scores[:seq_len]\n",
    "seq_expert_assign = seq_expert_assign[:seq_len]\n",
    "\n",
    "print(f\"\\n裁剪后的数据形状:\")\n",
    "print(f\"  seq_attn: {seq_attn.shape}\")\n",
    "print(f\"  seq_gate_scores: {seq_gate_scores.shape}\")\n",
    "print(f\"  seq_expert_assign: {seq_expert_assign.shape}\")\n",
    "\n",
    "# 计算每个位置的注意力权重总和（被其他位置关注的程度）\n",
    "attention_in = seq_attn.sum(axis=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"高注意力区域分析\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 找出高注意力区域\n",
    "attention_threshold = np.percentile(attention_in, 80)  # 前20%的高注意力位置\n",
    "high_attention_positions = np.where(attention_in > attention_threshold)[0]\n",
    "\n",
    "print(f\"注意力阈值 (前20%): {attention_threshold:.4f}\")\n",
    "print(f\"高注意力位置数量: {len(high_attention_positions)}\")\n",
    "print(f\"高注意力位置索引: {high_attention_positions.tolist()}\")\n",
    "\n",
    "print(f\"\\n高注意力位置详细信息:\")\n",
    "for i, pos in enumerate(high_attention_positions):\n",
    "    print(f\"  位置 {pos + 1}: {target_seq[pos]} (注意力分数: {attention_in[pos]:.4f})\")\n",
    "\n",
    "print(f\"\\n高注意力区域序列片段:\")\n",
    "high_attention_seq = ''.join([target_seq[i] for i in high_attention_positions])\n",
    "print(f\"  完整片段: {high_attention_seq}\")\n",
    "\n",
    "# 分析每个专家专注的位置\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"专家专注区域分析\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "num_experts = gate_scores.shape[-1]\n",
    "for expert_id in range(num_experts):\n",
    "    # 找出该专家被分配的位置\n",
    "    expert_positions = []\n",
    "    for pos in range(seq_len):\n",
    "        if expert_id in seq_expert_assign[pos]:\n",
    "            expert_positions.append(pos)\n",
    "\n",
    "    if expert_positions:\n",
    "        # 计算该专家专注位置与高注意力位置的重合度\n",
    "        overlap = len(set(expert_positions) & set(high_attention_positions))\n",
    "        overlap_ratio = overlap / len(expert_positions) if expert_positions else 0\n",
    "\n",
    "        print(f\"\\nExpert {expert_id}:\")\n",
    "        print(f\"  专注位置数量: {len(expert_positions)}\")\n",
    "        print(f\"  专注位置索引: {expert_positions}\")\n",
    "        print(f\"  专注位置氨基酸: {[target_seq[i] for i in expert_positions]}\")\n",
    "        print(f\"  专注区域序列: {''.join([target_seq[i] for i in expert_positions])}\")\n",
    "        print(f\"  与高注意力区域重合度: {overlap_ratio:.2f} ({overlap}/{len(expert_positions)})\")\n",
    "\n",
    "        # 分析该专家专注的氨基酸类型\n",
    "        aa_counts = Counter([target_seq[i] for i in expert_positions])\n",
    "        print(f\"  氨基酸分布: {dict(aa_counts)}\")\n",
    "\n",
    "        # 计算该专家专注位置的平均门控分数\n",
    "        avg_gate_score = np.mean([seq_gate_scores[pos, expert_id] for pos in expert_positions])\n",
    "        print(f\"  平均门控分数: {avg_gate_score:.4f}\")\n",
    "\n",
    "# 保存序列信息到文件\n",
    "with open('/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/sequence_analysis_info.txt', 'w') as f:\n",
    "    f.write(\"序列分析详细信息\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"序列索引: {seq_idx}\\n\")\n",
    "    f.write(f\"序列长度: {seq_len}\\n\")\n",
    "    f.write(f\"完整序列: {target_seq}\\n\")\n",
    "    f.write(f\"高注意力位置: {high_attention_positions.tolist()}\\n\")\n",
    "    f.write(f\"高注意力区域序列: {high_attention_seq}\\n\")\n",
    "\n",
    "print(f\"\\n序列信息已保存到: /exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/sequence_analysis_info.txt\")\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-17T03:25:21.695512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##### UniProt ID: A0A3M8QPJ6\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你已经有 attn_weights, pos_test_seqs, seq_idx, target_seq, seq_len\n",
    "import numpy as np\n",
    "\n",
    "# 假设你已经有 attn_weights, target_seq, seq_idx, seq_len\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 调整高注意力token的输出，减少数量，提高阈值\n",
    "def output_high_attention_tokens(attn_weights, target_seq, seq_idx, seq_len, output_txt, percentile=95):\n",
    "    with open(output_txt, 'w') as f:\n",
    "        f.write(f\"Sequence index: {seq_idx}\\n\")\n",
    "        f.write(f\"Sequence length: {seq_len}\\n\")\n",
    "        f.write(f\"Full sequence:\\n{target_seq}\\n\\n\")\n",
    "        for layer_idx in range(4):\n",
    "            attn = attn_weights[layer_idx, seq_idx]\n",
    "            if attn.ndim == 3:\n",
    "                attn = attn.mean(axis=0)\n",
    "            attn = attn[:seq_len, :seq_len]\n",
    "            attention_in = attn.sum(axis=0)\n",
    "            attention_threshold = np.percentile(attention_in, percentile)\n",
    "            high_idx = np.where(attention_in >= attention_threshold)[0]\n",
    "            high_aas = [target_seq[i] for i in high_idx]\n",
    "            f.write(\n",
    "                f\"Layer {layer_idx} high-attention tokens (threshold={attention_threshold:.4f}, top {len(high_idx)} tokens):\\n\")\n",
    "            f.write(f\"Indices: {high_idx.tolist()}\\n\")\n",
    "            f.write(f\"Amino acids: {''.join(high_aas)}\\n\\n\")\n",
    "            print(f\"Layer {layer_idx}: high-attention token indices: {high_idx.tolist()}\")\n",
    "            print(f\"Layer {layer_idx}: high-attention amino acids: {''.join(high_aas)}\")\n",
    "            print(f\"Layer {layer_idx}: number of high-attention tokens: {len(high_idx)}\")\n",
    "\n",
    "\n",
    "# 用法示例（提高阈值到90%，减少高注意力token数量）：\n",
    "# output_high_attention_tokens(attn_weights, target_seq, seq_idx, seq_len, '/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/high_attention_tokens_info.txt', percentile=90)\n",
    "for layer_idx in range(4):  # 4层\n",
    "    # 1. 取该层的注意力\n",
    "    attn = attn_weights[layer_idx, seq_idx]\n",
    "    if attn.ndim == 3:\n",
    "        attn = attn.mean(axis=0)  # [seq_len, seq_len]\n",
    "    attn = attn[:seq_len, :seq_len]\n",
    "\n",
    "    # 2. 注意力热图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attn, cmap='Blues', aspect='auto')\n",
    "    plt.title(f'Attention Weights (Layer {layer_idx})')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Token Position')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/attn_heatmap_layer{layer_idx}.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # 3. 注意力权重分布\n",
    "    attention_in = attn.sum(axis=0)\n",
    "    attention_threshold = np.percentile(attention_in, 80)\n",
    "    high_attention_positions = np.where(attention_in > attention_threshold)[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(seq_len), attention_in, 'b-', linewidth=2)\n",
    "    plt.axhline(y=attention_threshold, color='r', linestyle='--', label=f'Threshold ({attention_threshold:.4f})')\n",
    "    plt.scatter(high_attention_positions, attention_in[high_attention_positions],\n",
    "                color='red', s=50, zorder=5, label='High Attention Positions')\n",
    "    plt.title(f'Attention Incoming Weights (Layer {layer_idx})')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Sum of Incoming Attention')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/attn_incoming_layer{layer_idx}.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # 3.1 输出注意力权重最高的前5个token\n",
    "    top_n = 5\n",
    "    top_idx = np.argsort(attention_in)[-top_n:][::-1]  # 从大到小\n",
    "    top_aas = [target_seq[i] for i in top_idx]\n",
    "    top_values = attention_in[top_idx]\n",
    "    print(f\"Layer {layer_idx}: Top {top_n} high-attention token indices: {top_idx.tolist()}\")\n",
    "    print(f\"Layer {layer_idx}: Top {top_n} high-attention amino acids: {''.join(top_aas)}\")\n",
    "    print(f\"Layer {layer_idx}: Top {top_n} attention values: {top_values.tolist()}\")\n",
    "output_high_attention_tokens(attn_weights, target_seq, seq_idx, seq_len,\n",
    "                             '/exp_data/sjx/star/main_transformer_moe_weight/moe_analysis/high_attention_tokens_info.txt')\n"
   ],
   "id": "f728890133144f22",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
